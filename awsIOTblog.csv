,Title,Author,Published_date,Image,Link,Description
0,Optimize image classification on AWS IoT Greengrass using ONNX Runtime,b'Costin B\xc4\x83dici',2023-04-17T17:33:20+00:00,https://d2908q01vomqb2.cloudfront.net/f6e1126cedebf23e1463aee73f9df08783640400/2023/04/17/Optimize-image-classification-on-AWS-IoT-Greengrass-using-ONNX-Runtime.jpg-1024x576.png,https://aws.amazon.com/blogs/iot/optimize-image-classification-on-aws-iot-greengrass-using-onnx-runtime/,"b'Introduction\nPerforming machine learning inference on edge devices using models trained in the cloud has become a popular use case in Internet of Things (IoT) as it brings the benefits of low latency, scalability, and cost savings. When deploying models to edge devices with limited compute and memory, developers have the challenge to manually tune the model to achieve the desired performance. In this blog post, I will discuss an example on how to use the ONNX Runtime on AWS IoT Greengrass to optimize image classification at the edge.\nONNX is an open format built to represent any type of machine learning or deep learning model while making it easier to access hardware optimizations. It provides a standard format for interoperability between different machine learning frameworks. You can train an image classification model using one of your preferred frameworks (TensorFlow, PyTorch, MxNet, and more) and then export it to ONNX format. To maximize performance, you can use your ONNX models with an optimized inference framework, like ONNX Runtime. ONNX Runtime is an open source project designed to accelerate machine learning inference across a variety of frameworks, operating systems, and hardware platforms with a single set of APIs. While this blog post focuses on an example for image classification, you can use ONNX for a wide range of use cases, like object detection, image segmentation, speech and audio processing, machine comprehension and translation, and more.\nAWS IoT Greengrass is an open source Internet of Things (IoT) edge runtime and cloud service that helps you build, deploy, and manage IoT applications on your devices. You can use AWS IoT Greengrass to build edge applications using software modules, called components, that can connect your edge devices to AWS or third-party services. There are several AWS-provided machine learning components that can be used to perform inference on remote devices, with locally generated data, using models trained in the cloud. You can also build your custom machine learning components which can be divided in two categories: components for deploying and updating your machine learning models and runtimes at the edge as well as components that contain the necessary application logic for performing machine learning inference.\nSolution Overview\nIn this example, you will learn how to build and deploy a custom component for image classification on AWS IoT Greengrass. The below architecture and steps represent a possible implementation for this solution.\n1. Train a model using your preferred framework and export it to ONNX format, or use a pre-trained ONNX model. You can use Amazon SageMaker Studio and Amazon SageMaker Pipelines to automate this process.\nIn this blog post, you will be using a pre-trained ResNet-50 model in ONNX format for image classification available from the ONNX Model Zoo. ResNet-50 is a convolutional neural network with 50 layers and the pre-trained version of the model can classify images into a thousand object categories, such as keyboard, mouse, pencil, and many animals.\n2. Build and publish the necessary AWS IoT Greengrass components:\nAn ONNX Runtime component that contains the necessary libraries to run the ONNX model.\nA component for inference that contains the necessary code, the ResNet-50 model in ONNX format as well as some labels and sample images that will be used for classification. This component will have a dependency on the ONNX Runtime component.\n3. Deploy the component on the target device. Once the component is running, it will classify the sample images and publish the results back to AWS IoT Core to the topic demo/onnx. AWS IoT Core is a managed AWS service that let\xe2\x80\x99s you connect billions of IoT devices and route trillions of messages to AWS services without managing infrastructure.\nPrerequisites\nTo be able to run through the steps in this blog post, you will need:\nBasic knowledge of AWS IoT Core and AWS IoT Greengrass.\nBasic understanding of machine learning concepts.\nAn AWS Account.  If you don\xe2\x80\x99t have an AWS Account, follow the instructions to create one.\nAWS Command Line Interface (AWS CLI) and Git installed.\nAn AWS Identity and Access Management (IAM) user with the necessary permissions for creating and managing AWS resources.\nImplementation walkthrough\nInitial setup\nAs part of the initial setup for the environment, there are several resources that you need to provision. All the resources need to be provisioned in the same region. This guide is using the eu-central-1 region. Follow the steps below to get started:\n1. The component\xe2\x80\x99s artifacts are going to be stored in an Amazon Simple Storage Service (Amazon S3) bucket. To create an Amazon S3 bucket, follow the instructions from the user guide.\n2. To emulate a device where we will deploy the component, you will use an AWS Cloud9 environment and then install AWS IoT Greengrass client software. To perform these steps, follow the instructions from the AWS IoT Greengrass v2 workshop, sections 2 and 3.1.\n3. On the AWS Cloud9 environment, make sure you have python 3.6.9 as well as pip 23.0 or higher installed.\nBuild and publish the ONNX Runtime and inference components\nIn the next section, you will build and publish the custom components by using AWS CLI, either from a terminal on the local machine or in an AWS Cloud9 environment.\nTo upload the artifacts to the Amazon S3 bucket created as part of the initial setup, follow the next steps:\n1. Clone the git repository that contains the component\xe2\x80\x99s artifacts and recipe:\ngit clone https://github.com/aws-samples/aws-iot-gg-onnx-runtime.git\nBash\n2. Navigate to the artifacts folder and zip the files:\ncd aws-iot-gg-onnx-runtime/artifacts/com.demo.onnx-imageclassification/1.0.0 \nzip -r greengrass-onnx.zip .\nBash\n3. Upload the zip file to the Amazon S3 bucket that you created in the initial setup:\naws s3 cp greengrass-onnx.zip s3://{YOUR-S3-BUCKET}/greengrass-onnx.zip\nBash\nTo publish the components, perform the following steps:\n1. Open the recipe file aws-iot-gg-onnx-runtime/recipes/com.demo.onnx-imageclassification-1.0.0.json in a text editor. Below you have the command to navigate to the recipes directory:\ncd aws-iot-gg-onnx-runtime/recipes/\nBash\n2. Replace the Amazon S3 bucket name in artifacts URI with your own bucket name defined above:\n""Artifacts"": [\n    {\n      ""URI"": ""s3://{YOUR-S3-BUCKET}/greengrass-onnx.zip"",\n      ""Unarchive"": ""ZIP""\n    }\n  ]\nJSON\n3. Before publishing the component, make sure that you are using the same region where you created the resources in the initial setup. You can set your default region by using the following command:\naws configure set default.region eu-central-1\nBash\n4. Publish the ONNX Runtime component:\naws greengrassv2 create-component-version --inline-recipe fileb://com.demo.onnxruntime-1.0.0.json\nBash\n5. Publish the component that will perform the image classification and that has a dependency on the ONNX Runtime:\naws greengrassv2 create-component-version --inline-recipe fileb://com.demo.onnx-imageclassification-1.0.0.json\nBash\n6. To verify that the components were published successfully, navigate to the AWS IoT Console, go to Greengrass Devices >> Components. In the My Components tab, you should see the two components that you just published:\nDeploy the component to a target device\n1. To deploy the component to a target device, make sure that you have provisioned an AWS Cloud9 environment with AWS IoT Greengrass client software installed.\n2. To setup the necessary permissions for the Greengrass device, make sure that the service role associated with the Greengrass device has permissions to retrieve objects from the Amazon S3 bucket you previously created as well as permissions to publish to the AWS IoT topic demo/onnx.\n3. To deploy the component to the target device, go to the AWS IoT Console, navigate to Greengrass Devices >> Deployments and choose Create.\n4. Fill in the deployment name as well as the name of the core device you want to deploy to.\n\n5. In the Select Components section, select the component com.demo.onnx-imageclassification.\n6. Leave all other options as default and choose Next until you reach the Review section of your deployment and then choose Deploy.\n7. To monitor the logs and progress of the components\xe2\x80\x99 deployment, you can open the log file of Greengrass core device on the AWS Cloud9 environment with the following command:\nsudo tail -f /greengrass/v2/logs/greengrass.log\nBash\n8. Please note that the ONNX Runtime component, com.demo.onnxruntime, is automatically installed since the image classification component that we selected for deployment has a dependency on it.\nTest the ONNX image classification component deployment\nWhen the image classification component is in the running state, it will loop through the files in the images folder and it will classify them. The results are published to AWS IoT Core to the topic demo/onnx.\nTo understand this process, let\xe2\x80\x99s have a look at some code snippets from the image classification component:\n1. To check the sample images so that you can later compare them with the predicted labels, please open the images located in aws-iot-gg-onnx-runtime/artifacts/com.demo.onnx-imageclassification/1.0.0/images folder.\n2. The predict function shown below starts an inference session using the ONNX Runtime and the pre-trained ResNet-50 neural network in ONNX format.\ndef predict(modelPath, labelsPath, image):\n    labels = load_labels(labelsPath)\n    # Run the model on the backend\n    session = onnxruntime.InferenceSession(modelPath, None)\nPython\n3. The image is initially preprocessed and then passed as an input parameter to the inference session. Please note that ResNet-50 model uses images of 224 x 224 pixels.\nimage_data = np.array(image).transpose(2, 0, 1)\ninput_data = preprocess(image_data)\nstart = time.time()\nraw_result = session.run([], {input_name: input_data})\nend = time.time()\nPython\n4.  From the inference result, you extract the label of the image, and you also calculate the inference time in milliseconds.\ninference_time = np.round((end - start) * 1000, 2)\nidx = np.argmax(postprocess(raw_result))\ninferenceResult = {\n ""label"": labels[idx],\n ""inference_time"": inference_time\n}\nPython\n5. The image classification component loops through the files present in the images folder and invokes the predict function. The results are published to AWS IoT Core to the demo/onnx topic every 5 seconds.\nfor img in os.listdir(imagesPath):\n        request = PublishToIoTCoreRequest()\n        request.topic_name = topic\n        image = Image.open(imagesPath + ""/"" + img)\n        pred = predict(modelPath, labelsPath, image)\n        request.payload = pred.encode()\n        request.qos = qos\n        operation = ipc_client.new_publish_to_iot_core()\n        operation.activate(request)\n        future_response = operation.get_response().result(timeout=5)\n        print(""successfully published message: "", future_response)\n        time.sleep(5)\nPython\nTo test that the results have been published successfully to the topic, go to AWS IoT Console, navigate to MQTT Client section and subscribe to the topic demo/onnx. You should see the inference results like in the screenshot below:\nCleaning up\nIt is a best practice to delete resources you no longer want to use. To avoid incurring additional costs on your AWS account, perform the following steps:\n1. Delete the AWS Cloud9 environment where the AWS IoT Greengrass software was installed:\naws cloud9 delete-environment --environment-id <your environment id>\nBash\n2. Delete the Greengrass core device:\naws greengrassv2 delete-core-device --core-device-thing-name <thing-name>\nBash\n3. Delete the Amazon S3 bucket where the artifacts are stored:\naws s3 rb s3://{YOUR-S3-BUCKET} --force\nBash\n'"
1,Protecting Linux-based IoT devices against unintended USB access,b'Costin B\xc4\x83dici',2023-04-17T17:33:20+00:00,https://d2908q01vomqb2.cloudfront.net/f6e1126cedebf23e1463aee73f9df08783640400/2023/04/14/Protecting-Linux-based-IoT-devices-against-unintended-USB-access.jpg-1024x576.png,https://aws.amazon.com/blogs/iot/protecting-linux-based-iot-devices-against-unintended-usb-access/,"b'Introduction\nThe Internet of Things (IoT) industry continues to grow at an aggressive pace. As the number of connected devices ramps up, managing security of these devices at scale can be challenging. One of the potential risks that IoT devices face is unintended USB access, which can occur when an unauthorized connection is made through its USB port. For example, if a user gains physical access to a security camera system, there\xe2\x80\x99s a risk they could plugin an unauthorized USB device that provides access to the camera system and its data. This could result in unauthorized access to sensitive data or a disruption in system availability.\nProtecting IoT devices from unintended USB access requires a multi-layered security approach that includes both hardware and software solutions. Hardware solutions include implementing an additional layer of security to the USB ports and limiting physical access to devices. Software solutions include implementing firmware and software updates, as well as implementing security protocols that can detect and prevent unintended USB access.\nThe access level for a device can also be different depending on whether it is in service or in debug mode. When a device is in service, you may want its USB ports to be fully protected. When it is in debug mode, you sometimes need to open up its USB ports to allow a technician to plug in diagnosis software. The control over the device\xe2\x80\x99s mode needs to be securely performed by a security or DevOps team, as shown in Figure 1.\nIn this blog, you will learn how to protect Linux-based IoT devices and computers against unintended USB access with USBGuard and how to securely change a device from In-Service mode to Debug mode with AWS IoT Device Management.\nFigure 1: Use case explanation\nPrerequisites\nAn AWS account. If you do not have an AWS account, you will need to create and activate an AWS account first and set up both AWS IoT Core and AWS IoT Device Management. For getting started instructions, please view the AWS IoT Core getting started guide and the guide for how to manage devices with AWS IoT.\nTurn on AWS IoT Core fleet indexing, which is required for use with Fleet Hub for AWS IoT Device Management.\nIntegrated with AWS IoT Core, AWS IoT Device Management helps you register, organize, monitor, and remotely manage IoT devices at scale. With Fleet Hub you can build standalone web applications for monitoring the health of your device fleets.\nA Linux based physical machine with USB ports. Nvidia Jetson Nano is used in this blog.\nBasic understanding of Linux (e.g. create directories, set file permissions) and programming (compiling code)\nWalkthrough\nThe following diagram, Figure 2, shows an architecture of a Linux-based device connecting through AWS IoT Core using MQTT. On the device, the USBGuard service has been installed and enabled. USBGuard is a software framework that offers an allow/deny -listing mechanism for USB-devices. Inspiration for this is drawn from issues like BadUSB. It makes use of a device blocking infrastructure included in the Linux kernel.\nThe device has a device mode attribute defined. You can set the device mode to either in-service or debug mode through Jobs for AWS IoT. IoT Jobs define a set of remote operations that can be sent to and run on one or more devices connected to AWS IoT. For this use case, there are two jobs defined: set-debug-mode-job and set-in-service-mode-job. You can monitor device mode attributes and perform jobs through AWS IoT Device Management Fleet Hub.\nWhen running the set-in-service-mode-job, the IoT communication client will pick up the job, set a USBGuard policy to restrict USB ports access only to designated USB device, block other devices with a hidden keyboard interface in a USB flash disk, and set the device shadow attribute with in-service mode . On the contrary, when running the set-debug-mode-job, it loosens the rules on the USB ports (e.g. a USBGuard policy to allow all USB ports access), and sets the device shadow attribute with debug mode. This way, a technician can plug in a mouse and a keyboard and run debugging software through USB ports.\nFigure 2: Solution architecture\nThis can be accomplished through the following procedures:\nConfigure AWS IoT Core policy and device attributes\nProvision the device\nInstall USBGuard on the device\nImplement the IoT communication client code\nConfigure job docs in IoT Jobs\nCreate AWS IoT Device Management Fleet Hub application\nSteps 1 and 2 can be performed either in AWS IoT Core console or through AWS CLI. We use AWS CLI commands in the walkthrough. Step 3 and 4 are configured on the IoT device. And step 5 and 6 are performed in AWS IoT console.\nStep 1: Configure AWS IoT Core policy and device attributes\nPOLICY_NAME=IoTJobDemo_Policy\nTHING_TYPE_NAME=DemoDevice\n\n# Create an IoT policy\n# NOTE: This policy is for demonstration purpose only! Please do not use in production environment.\n# Replace us-east-1:123456789012 with your AWS_REGION:AWS_ACCOUNT_ID\n# Replace uniqueThingName with your IoT device thing name\naws iot create-policy --policy-name $POLICY_NAME --policy-document \'{\n  ""Version"": ""2012-10-17"",\n  ""Statement"": [\n    {\n      ""Effect"": ""Allow"",\n      ""Action"": ""iot:Connect"",\n      ""Resource"": ""arn:aws:iot:us-east-1:123456789012:client/uniqueThingName""\n    },\n    {\n      ""Effect"": ""Allow"",\n      ""Action"": ""iot:Publish"",\n      ""Resource"": [\n        ""arn:aws:iot:us-east-1:123456789012:*""\n      ]\n    },\n    {\n      ""Effect"": ""Allow"",\n      ""Action"": ""iot:Subscribe"",\n      ""Resource"": ""arn:aws:iot:us-east-1:123456789012:*""\n    },\n    {\n      ""Effect"": ""Allow"",\n      ""Action"": ""iot:Receive"",\n      ""Resource"": [\n        ""arn:aws:iot:us-east-1:123456789012:topic/test/dc/subtopic"",\n        ""arn:aws:iot:us-east-1:123456789012:topic/$aws/things/uniqueThingName/jobs/*""\n      ]\n    },\n    {\n      ""Effect"": ""Allow"",\n      ""Action"": [\n        ""iot:DescribeJobExecution"",\n        ""iot:GetPendingJobExecutions"",\n        ""iot:StartNextPendingJobExecution"",\n        ""iot:UpdateJobExecution""\n      ],\n      ""Resource"": ""arn:aws:iot:us-east-1:123456789012:topic/$aws/things/uniqueThingName""\n    }\n  ]\n}\n\'\n\n\n# Create thing type\naws iot create-thing-type --thing-type-name $THING_TYPE_NAME\n\n# Create dynamic thing groups\naws iot create-dynamic-thing-group --thing-group-name ""Debug"" --query-string ""shadow.name.device-mode.reported.mode:Debug"" > /dev/null\naws iot create-dynamic-thing-group --thing-group-name ""InService"" --query-string ""shadow.name.device-mode.reported.mode:InService"" > /dev/null\nBash\nStep 2: Provision the device\nTHING_NAME=""<your device unique identifier>""\nTHING_PATH=""<your work directory>/$THING_NAME""\n\naws iot create-thing --thing-name $THING_NAME --thing-type-name $THING_TYPE_NAME \n\n# Create keys and certificate\naws iot create-keys-and-certificate --set-as-active \\\n    --private-key-outfile $THING_PATH/private.key \\\n--certificate-pem-outfile $THING_PATH/certificate.pem > $THING_PATH/keys_response\n\n# Get Root CA\nwget https://www.amazontrust.com/repository/AmazonRootCA1.pem -O $THING_PATH/rootCA.pem\n\n# Parse output for certificate ARN and ID\nCERTIFICATE_ARN=$(jq -r "".certificateArn"" $THING_PATH/keys_response)\nCERTIFICATE_ID=$(jq -r "".certificateId"" $THING_PATH/keys_response)\n# Attach policy to certificate\naws iot attach-policy --policy-name $POLICY_NAME --target $CERTIFICATE_ARN\n# Attach certificate to thing\naws iot attach-thing-principal --thing-name $THING_NAME --principal $CERTIFICATE_ARN\nBash\nStep 3: Install USBGuard on the device\nFollow the public documentation to install USBGuard.\nGenerate two USBGuard policies, debug-rules.conf and in-service-rules.conf. in-service-rules.conf contains strict rules and will be used in device\xe2\x80\x99s in-service mode. debug-rules.conf loosens the rules on the USB ports and will be used in device\xe2\x80\x99s debug mode.\nFor example, a debug-rules.conf can be a policy allowing all access to all USB ports:\necho ""allow id *:*"" > debug-rules.conf\nBash\nin-service-rules.conf can contain rules to reject any USB flash disk which implements a keyboard or a network interface.\nallow with-interface equals { 08:*:* }\nreject with-interface all-of { 08:*:* 03:00:* }\nreject with-interface all-of { 08:*:* 03:01:* }\nreject with-interface all-of { 08:*:* e0:*:* }\nreject with-interface all-of { 08:*:* 02:*:* }\nBash\nStep 4: Implement the IoT communication client code\nOn the IoT device itself, create a work directory:\nWORKDIR=""<your work directory>""\nTHING_PATH=""$WORKDIR/$THING_NAME""\nmkdir -p $THING_PATH\nBash\nPut the certificate.pem, private.key, rootCA.pem from Step 2 under the $THING_PATH.\nWe use the AWS IoT Device Client on the device. The AWS IoT Device Client is free, open-source, modular software written in C++ that you can compile and install on your Embedded Linux based IoT devices to access AWS IoT Core, AWS IoT Device Management, and AWS IoT Device Defender features by default. To install and configure the client:\n# Building\ncd $WORKDIR\ngit clone https://github.com/awslabs/aws-iot-device-client\ncd aws-iot-device-client\nmkdir build\ncd build\ncmake ../\ncmake --build . --target aws-iot-device-client\n\n# Setup\ncd ../\n./setup.sh \nBash\nAt this point you\xe2\x80\x99ll need to respond to prompts for information, including paths to your thing certs:\nDo you want to interactively generate a configuration file for the AWS IoT Device Client? y/n\ny\nSpecify AWS IoT endpoint to use:\n<This is the iot:Data-ATS endpoint. Check out https://docs.aws.amazon.com/iot/latest/developerguide/iot-connect-devices.html>\nSpecify path to public PEM certificate:\n$THING_PATH/certificate.pem\nSpecify path to private key:\n$THING_PATH/private.key\nSpecify path to ROOT CA certificate:\n$THING_PATH/rootCA.pem \nSpecify thing name (Also used as Client ID):\n$THING_NAME\nWould you like to configure the logger? y/n\nn\nEnable Jobs feature? y/n\ny\nSpecify absolute path to Job handler directory:\n$WORKDIR/.aws-iot-device-client/jobs\n\xe2\x80\xa6\nEnable Sample Shadow feature? y/n\ny\nSpecify a shadow name for the feature to create or update:\ndevice-mode\nSpecify the path of a file for the feature to read from:\n$WORKDIR/.aws-iot-device-client/device-mode-input.json      \nSpecify a the path of a file for the feature to write shadow document to:\n$WORKDIR/.aws-iot-device-client/device-mode-output.json           \n\xe2\x80\xa6\nDo you want to install AWS IoT Device Client as a service? y/n\nN\nBash\nIn the $WORKDIR/.aws-iot-device-client/jobs, create a usbguard-policy directory and put the debug-rules.conf and in-service-rules.conf generated from step 3 in there.\nCreate two corresponding job handler scripts in the $WORKDIR/.aws-iot-device-client/jobs to handle set device mode. They will be triggered by IoT Job docs.\nThe script to set device to debug mode: set-debug-mode.sh\n#!/usr/bin/env sh\nset -e\n\nBASEDIR=$(dirname $0)\necho \'{""mode"": ""Debug""}\' > $BASEDIR/../device-mode-input.json\ncat $BASEDIR/usbguard-policy/debug-rules.conf > /etc/usbguard/rules.conf\nsystemctl restart usbguard.service\nBash\nThe script to set device to in-service mode: set-in-service-mode.sh\n#!/usr/bin/env sh\nset -e\n\nBASEDIR=$(dirname $0)\necho \'{""mode"": ""InService""}\' > $BASEDIR/../device-mode-input.json\ncat $BASEDIR/usbguard-policy/in-service-rules.conf > /etc/usbguard/rules.conf\nsystemctl restart usbguard.service\nBash\nNow, everything has been configured on the device, we can run the device client by:\ncd $WORKDIR/aws-iot-device-client\nsudo ./build/aws-iot-device-client\nBash\nStep 5: Configure job docs in IoT Jobs\nTwo job docs need to be created in IoT Jobs. set_debug_mode.json and set_in_sevice_mode.json. They will trigger the set-debug-mode.sh and set_in_sevice_mode.sh handlers that we wrote in Step 4 respectfully.\nExample of set_debug_mode.json:\n{\n  ""_comment"": ""This sample JSON file can be used for set debug mode."",\n  ""version"": ""1.0"",\n  ""steps"": [\n    {\n      ""action"": {\n        ""name"": ""Set Device Mode to Debug"",\n        ""type"": ""runHandler"",\n        ""input"": {\n          ""handler"": ""set-debug-mode.sh""\n        },\n        ""runAsUser"": ""root""\n      }\n    }\n  ]\n}\nJSON\nCreate an Amazon S3 bucket and upload the job docs. In AWS IoT Jobs console, configure the Job templates with the two job docs:\nStep 6: Create AWS IoT Device Management Fleet Hub application\nFollow the blog post Get Started with Fleet Hub for AWS IoT Device Management to set up Fleet Hub. On your Fleet Hub application, you should be able to see your device and its mode.\nYou can also control to change the mode by run jobs from the dashboard.\nOn the device, you should be able to observe that access to any USB port is corresponding to the rules you have set for the device mode.\nCleaning up\nTo avoid incurring future charges, delete all resources that you have created.\nThe Fleet Hub application can be deleted by first navigating to Fleet Hub in the AWS IoT console and then selecting applications. Select your application and choose delete.\nAWS IoT Core Fleet indexing can be turned off by navigating to the AWS IoT Core console, selecting Settings, then navigating to Manage fleet Indexing and then Thing indexing and group indexing.\nIn the AWS IoT Core console, delete Things, Thing groups and Thing types under All devices. Detach \xe2\x80\x98thing\xe2\x80\x99 and IoT policy from registered certificate. Delete device certificate, \xe2\x80\x98thing,\xe2\x80\x99 and IoT policy.\n'"
2,Deploying and managing an IoT workload on AWS,b'Costin B\xc4\x83dici',2023-04-17T17:33:20+00:00,https://d2908q01vomqb2.cloudfront.net/f6e1126cedebf23e1463aee73f9df08783640400/2023/04/14/Deploying-and-managing-an-IoT-workload-on-AWS.jpg-1024x576.png,https://aws.amazon.com/blogs/iot/deploying-and-managing-an-iot-workload-on-aws/,"b'Introduction\nWhen implementing an Internet of Things (IoT) workload, companies are faced with multiple options when it comes to choosing a platform. From building it entirely from scratch, including your own device hardware, all the way to purchasing preconfigured hardware and just connecting to a completely Software as a service (SaaS) IoT platform.\nThe goal of this blog is to help you understand what skills and knowledge are required for designing an IoT solution and help you decide what components you would want to build versus buy. If you\xe2\x80\x99re thinking of migrating your IoT workload to AWS, then please review the Planning a Seamless Migration to AWS IoT Core blog as a first step to understand key reasonings, incentives, and support offered by AWS that can help simplify your migration process.\nCommon AWS IoT architecture components\nDevice manufacturing\nWhen developing and manufacturing device hardware, there are several factors to consider. Based upon your requirements, hardware must be selected to meet the current and future needs of your solution. Decisions must be made in regards to common IoT constraints such as managing power (supply and consumption), connectivity, security, and operating system.\nIf you are not building hardware in-house, then an Original Device Manufacturer (ODM) will need to be chosen. ODMs have the production line, tooling, and processes in place to produce large volumes of devices. They are able to build to the specification provided by you, which typically includes the printed circuit board (PCB) schematics, a bill of materials, firmware, and provisioning requirements.\nConsiderations for device hardware constraints include:\nPower consumption: How and where devices are to be used has a large impact on how they will be powered. A wearable device will require a small battery whereas a television will be able to leverage an AC power supply. For devices requiring batteries, you need to determine if they will be rechargeable, replaceable, or expected to last the life-time of the hardware.\nOperating system and firmware: The selection of an operating system or firmware will depend on the type of device and the tasks it is expected to perform. Small, low-power devices could require a real-time operating system, such as FreeRTOS, whereas larger, dedicated-power devices may utilize a full-stack operating system such as Linux.\nConnectivity: There are a multitude of connectivity and protocol options for IoT solutions, such as Ethernet, Wi-Fi, Cellular, LoRaWAN, and Bluetooth Low Energy (BLE). Device geography, availability, power consumption, security, and use case will determine which connectivity option is best for your solution.\nTo help with this component, AWS offers the AWS Partner Device Catalog, which offers a list of AWS partner manufactured devices that have completed the AWS Device Qualification Program. Devices from this list can help you go to market faster and ensure your device is compatible with AWS IoT and AWS best practices. In addition, if you\xe2\x80\x99ve manufactured your own devices, you can use the AWS IoT Core Device Advisor to validate their ability to reliably and securely connect with AWS IoT Core.\nDevice provisioning\nHow you provision devices in your IoT solution will vary based on the capabilities of your device and its manufacturing process. The main focus here is on how your device and its credentials are created.\nSecurity should be a high priority for you, your customers, and device manufacturers. When using X.509 certificates, the manufacturing process must specify when devices will receive their unique certificate and private key pairing as well as how they will be registered in your IoT solution.\nConsiderations for device provisioning and certificate management include:\nManufacturer selection: A complete certificate chain of trust starts when you develop hardware in-house or select an OEM partner. If going with the latter, their processes will need to be inspected to ensure that certificate integrity is maintained throughout their supply chain.\nCertificate Authority (CA): To provide flexibility in the manufacturing of device, AWS has several options available including using your own CA, a third-party CA, or the Amazon Root certificate authority (CA).\nHardware security module: Secure elements built into IoT devices form the basis for device security. This enables encryption and tamper-proof storage of certificates and secrets and firmware and applications to be validated. To help with this, AWS has a range of connectivity modules powered by AWS IoT ExpressLink which include software implementing AWS mandated security requirements.\nExternal resources: Resources may need to be created in your IoT solution to enable a custom provisioning process. These resources have to be designed to scale as your device fleet grows. With AWS, this could be an AWS Lambda function that acts as a Pre-provisioning hook.\nDevice-level logic: A device may require on-device logic to successfully, reliably, and securely be provisioned. With AWS, the AWS IoT SDKs have been built to enable this on-device logic.\nFor more information on provisioning and registering devices securely with AWS IoT Core, please review the Device Manufacturing and Provisioning with X.509 Certificates in AWS IoT Core AWS whitepaper and the AWS IoT Core Device Provisioning documentation.\nDevice management\nWith a mature provisioning process, a device can be secure and up-to-date from the first time it connects but it may require updates, such as firmware or certificate rotation, to remain fully compliant and provide the best user experience. Solutions for these updates will need to be designed to react to interruptions in delivery, connectivity, rollback routines, and to scale automatically.\nConsiderations for your device management strategy include:\nOrganize devices: The ability to quickly identify and interact with devices gives you the ability to troubleshoot and potentially isolate them if they become out of compliance. When operating fleets of devices, you need to have solutions in-place to organize, index, and categorize your devices at scale. With AWS, you could use Fleet Hub for AWS IoT Device Management.\nMonitor devices: Monitoring the status of your device fleet is important in helping identify any malfunctioning or out-of-compliance devices. Ensure you have a monitoring solution in place to collect observational and security data, such as device metrics, logs, or configuration. AWS IoT Device Defender provides auditing and ongoing intelligent monitoring for security of your fleet.\nRespond to events: By defining a minimum set of logs, metrics, and alarms, your operations team can defend against significant business interruptions. A scalable alerting solution that integrates with your monitoring solution will be required for this. With AWS, you could use Amazon CloudWatch.\nEnable Over-The-Air (OTA) Updates: Devices should be designed to receive and apply updates. Your IoT solution should be designed to send updates and monitor a device\xe2\x80\x99s update progress. With AWS, you could use AWS IoT Device Management Jobs.\nTo help with this component, AWS IoT Device Management, AWS IoT Device Defender, and AWS IoT Core offer a full set of capabilities to address device organization, monitoring, alerting, and OTA updates across your fleet of IoT devices.\nDevice data ingestion\nNot all IoT solutions will focus just on data ingestion, but for the ones that do, this will be a primary component that affects the solution\xe2\x80\x99s entire architecture. The requirements for this component will affect your solution\xe2\x80\x99s scale, cost, security, and performance which means you should design your IoT solution\xe2\x80\x99s architecture to meet your current and potential future data ingestion.\nConsiderations for your data ingestion strategy include:\nData size: Assuming your devices are not hardware constrained, for optimal efficiency, try to keep the size of your messages consistent and consider batching of smaller messages to accomplish this. Keep in mind, batching can occur on and after message transmission such as batching messages using IoT Rules after they\xe2\x80\x99ve been ingested by IoT Core.\nData frequency & structure: Consider how often your devices transmit messages and if your solution is designed to scale for this. In addition to frequency, the structure of your data will determine if your IoT workload is messaging or streaming based.\nMQTT topic design: If you\xe2\x80\x99re using this protocol, you should strive to find a balance between a schema that enforces least privilege communication and also allows for supporting future device deployments. A good topic schema will implement a common naming structure to provide for flexible message filtering and message routing.\nData storage: Analyze the flow and usage of your messages to identify the right storage solutions. These storage solutions will have multiple considerations such as your specific use case, overall message structure, scale (for current and future growth), and cost.\nRouting: Once ingested, you\xe2\x80\x99ll need an easy, rules-based solution to route messages to either storage or other services. These rules can then be used for further message batching, processing, or even alerting.\nEdge Gateway: A common architecture pattern is to have a gateway, or broker, for ingesting, processing, and/or batching data before transmitting to your IoT solution. This can be implemented as either a local endpoint, closer to your devices, or cloud, closer to your IoT solution, based gateway.\nTo help with this component, AWS IoT Core enables you to connect billions of IoT devices and route trillions of messages to other AWS services, such as Amazon SQS, Amazon Kinesis, and Amazon SNS, without managing any infrastructure. AWS also offers AWS IoT Greengrass which is an open-source edge runtime that provides the capabilities of an edge gateway. For more information on patterns for data ingestion with AWS IoT Core, please refer to the AWS IoT blog 7 patterns for IoT data ingestion and visualization- How to decide what works best for your use case.\nReal-time video and data streams\nIn addition to the items discussed in the previous section, you will need to consider a few more if your IoT workload consists of video or other high volume data streams. An IoT workload that handles streams of data typically deals with high frequencies and raw, unstructured data for applications such as video processing and analysis.\nConsiderations streaming based workloads include:\nProducing: How your data streams are produced can directly affect how they are ingested, processed and stored in your IoT solution downstream. Aspects such as your device\xe2\x80\x99s streaming protocol, network availability, accessibility and cost constraints will affect how your streams are produced.\nConsuming: The consumption and processing of your data streams can affect the required scale and overall cost of your IoT solution. High frequencies of data, such as video streams, will lead to the need for a robust architecture that is highly available, easy to manage, and can handle your throughput requirements. Consider the direct business value of these streams in your overall IoT solution to determine the most cost-effective and scalable way to consume and process them.\nTo help with this type of architecture, AWS offers AWS IoT Greengrass, Amazon Kinesis, and Amazon Kinesis Video Streams. AWS IoT Greengrass is an open-source edge runtime that provides the capabilities to easily consume and process data streams at the edge and transfer them to AWS via AWS-provided components. Amazon Kinesis is a cost-effective, managed service that can process and analyze streaming data produced either directly from a device, the AWS IoT Greengrass Stream manager component or an AWS IoT Rule. Amazon Kinesis Video Streams is a managed AWS service that can be used to securely view, process and analyze video streams produced either directly by a device or the AWS IoT Greengrass Edge connector for Kinesis Video Streams, regardless of the source protocol.\nDevice command-and-control\nCommand-and-control is the operation of sending a message to a device requesting it to perform an action with an optional acknowledgement of success or failure. This can be accomplished with either a command message to your device or by changing and relaying your device\xe2\x80\x99s state from your IoT solution. Evaluating and optimizing your IoT solution\xe2\x80\x99s messaging needs for data ingestion versus command-and-control ensures that you get the best outcomes in balancing performance and cost.\nConsider the following patterns for your device command-and-control strategy:\nCommand messaging: Use direct device message(s) with your messaging protocol of choice to transmit command(s) directly to a device. You will need device-level logic in place to accept and execute the command as well as report the device\xe2\x80\x99s execution status. Please be aware that this pattern will require your IoT solution to ensure the command message is delivered or results in an actionable failure should your device be offline or disconnected.\nDevice state: A device\xe2\x80\x99s persisted state will need to be handled by your IoT solution and can be used to set device commands and update their execution status. This persisted state could be a simple document that is sent to the device when changes are made from the IoT solution and sent back if the device makes changes as well. This pattern will allow your IoT solution to interact with your device, whether it\xe2\x80\x99s connected or not.\nTo help with this component, AWS IoT Core offers the AWS IoT Device Shadow service, the MQTT5 request/response pattern, and AWS IoT Device Management offers the AWS IoT Jobs feature. For more information on patterns for implementing device command-and-control, please see the Device Commands section of the AWS IoT Lens for the AWS Well-Architected Framework whitepaper.\nCloud architecture\nWhen an IoT solution exists in the cloud, you may start with one regional service or with a small fleet of devices to test with your requirements. This will be fine for proof-of-concepts or demonstrations, but when you move the solution into production you need to ensure it\xe2\x80\x99s built with cloud-based best practices in mind.\nThe AWS Well-Architected framework can help you in the design, build or even review of your solution to ensure it is using AWS in a secure, high-performing, resilient, and efficient manner. For more information on cloud based best practices with AWS IoT, please see the IoT Lens \xe2\x80\x93 AWS Well-Architected Framework.\n'"
3,Secure IIoT secondary sensing using AWS Snowcone and CloudRail,b'Costin B\xc4\x83dici',2023-04-17T17:33:20+00:00,https://d2908q01vomqb2.cloudfront.net/f6e1126cedebf23e1463aee73f9df08783640400/2023/04/11/Secure-IIoT-secondary-sensing-using-AWS-Snowcone-and-CloudRail.jpg-1024x576.png,https://aws.amazon.com/blogs/iot/secure-iiot-secondary-sensing-using-aws-snowcone-and-cloudrail/,"b""Introduction\nOne of the major barriers to Industrial IoT (IIoT) adoption is integrating modern IIoT solutions in brownfield environments with legacy components and systems. These legacy industrial components and systems could be 20, 30, 40 years old and are less capable of supporting modern security standards. Physically connecting legacy industrial systems to the cloud can be complex, costly, and time-consuming. Secondary sensing refers to equipping older machines (brownfield) with additional sensors to gather data for IIoT applications. We discussed secondary sensing and actuation for factories using AWS IoT and CloudRail Gateways. In this blog post, we provide guidance on an alternate approach and discuss the benefits of a secondary sensing solution using AWS Snowcone (Snowcone) running CloudRail.OS Docker application. This solution is a non-invasive, secure, and cost-effective way to collect and send OT data from brownfield environments to AWS IoT SiteWise without impacting safety and plant operations.\nBackground\nTo enable IIoT applications for improving operational efficiencies, reducing unplanned downtime, and improving product quality, data from machines and industrial equipment needs to be acquired and transferred to the edge and cloud for processing. A mixture of legacy and modern equipment, as well as a variety of different protocols can make this connectivity difficult to establish. Furthermore, industrial organizations are facing a new challenge as they try to merge the traditional physical world (Operational Technology or OT) and the digital world (Information Technology or IT). This is discussed in Managing Organizational Transformation for Successful OT/IT Convergence.\nIntroducing IIoT in brownfield environments can open new avenues for cyber-events and needs additional security consideration since it can result in connecting \xe2\x80\x9cinsecure by design legacy industrial control (ICS/OT) systems\xe2\x80\x9d to external and untrusted networks like the internet. In brownfield IIoT deployments, new IIoT technologies co-exists with legacy brownfield systems. This integration of IT and OT introduces risk since systems built for usage in hostile networks are integrated with those that were not.  IIoT has significantly widened the array of technologies available for use in industrial environments like secondary sensors. OT/IT convergence and the growth of IIoT increases the attack surface, which inherently increases the risk of compromise in these environments. For brownfield environments, AWS recommends following the Ten Security Golden Rules for IIoT solutions.\nSolution architecture and components\n The architecture enclosed shows a secondary sensing solution using CloudRail.OS running on an AWS Snowcone acting as an edge gateway. An IO-Link Master is used to connect temperature and vibration IO-Link sensors to CloudRail.OS on Snowcone. Sensor data is securely sent to AWS IoT SiteWise in the AWS Cloud.\nFigure 1: Secondary sensing architecture using CloudRail.OS on AWS Snowcone\nA brief description of the solution components is as follows:\nAWS Snowcone\nAWS Snowcone is a small, rugged, and secure device offering edge computing and local data storage, in environments with little or no connectivity to the AWS Region. Snowcone is used to run IIoT applications in austere (non-data center) industrial edge environments. With 2 vCPUs, 4 GB of memory, and 8 TB of usable storage (14 TB for Snowcone SSD), Snowcone devices can come provisioned with several AWS services, including Amazon EC2, AWS NFS, and Amazon EBS, for secure, ruggedized data storage and compute ideal for IIoT and factory floor uses. Snowcone\xe2\x80\x99s small size (8.94 inches long x 5.85 inches wide x 3.25 inches tall / 227 mm x 148.6 mm x 82.65 mm) enables you to set it next to machinery in a factory to collect, format, and transport data back to AWS for storage and analysis. All data on the Snowcone is always automatically encrypted and the Trusted Platform Module (TPM) provides hardware root of trust. Snowcone simplifies OT/IT integration by securely bridging OT and IT networks.\nCloudRail\nCloudRail is a fully managed plug-and-play solution to acquire data from industrial environments, pre-process it locally, and send it to AWS IoT Core, AWS IoT SiteWise, or AWS IoT Greengrass. CloudRail works for greenfield as well as brownfield applications. It uses industry standards like OPC-UA to connect modern equipment, while old machines are retrofitted with secondary sensors. A database of over 12,000 sensor definitions in combination with automated data transformation and device provisioning reduces the setup time for connecting a machine to the cloud from weeks to just hours. The optional support of AWS IoT Greengrass runs powerful logic locally on the edge device like data pre-processing or machine learning applications.\nCloudRail.OS provides a container-based Docker application which runs on the Snowcone.\nBy combining CloudRail\xe2\x80\x99s plug-and-play approach for connecting industrial assets to the cloud with the AWS Snowcone\xe2\x80\x99s secure and rugged compute and storage offering, customers get an industrial-grade ruggedized solution. Due to the deep integration of CloudRail with AWS IoT services, data acquisition is simple, cost effective and scalable. The solution enables customers to quickly, easily, and securely collect OT data from brownfield environments to implement IIoT use cases.\nIO-Link\nIO-Link is a serial digital communication protocol used in industrial automation systems. It connects sensors and actuators to a programmable logic controller (PLC) and is a PLC standard for a serial communication protocol that allows three types of data to be exchanged \xe2\x80\x93 process data, service data, and events.\nIO-Link uses point-to-point connectivity between an IO-Link Master device and sensors rather than a message bus topology. Multiple IO-Link Masters can be connected to the Snowcone gateway box via an Ethernet connection. This allows a single gateway to support sensors and actuators across longer runs within a factory floor. Hundreds of IO-Link based sensors and actuators are supported by vendors such as IFM, Turck, Sick, Pepperl+Fuchs, or Balluff. IO-Link Design Guide can be used in designing IIoT solutions using IO-Link sensors and actuators.\nSome of the benefits of the CloudRail.OS on AWS Snowcone IIoT secondary sensing solution are:\nIoT plug-and-play support for industrial secondary sensors and support for thousands of IO-Link sensors\nReduce the time to connect an industrial machine to AWS\nStart small and quickly scale based on your learnings\nRuggedized and industrial-grade AWS managed gateway appliance with AWS Snowcone\nImprove security with AWS Snowcone security features including TPM, for hardware root of trust and data encryption at rest by default using 256-bit keys\nSimplify OT/IT convergence by securely bridging OT and IT networks\nImprove safety and reduce downtime when adding secondary sensing to production sites without impacting production\nOptionally add security audit and monitoring using AWS IoT Device Defender to audit for security best practices and monitor for device anomalies\nSolution Configuration\nWe will provide steps to build the architecture diagram mentioned above (Figure 1). The steps will guide you from ordering Snowcone to setting up Cloudrail.OS on an EC2 instance running on Snowcone.\nI. Prerequisite steps:\nProcured sensors from your manufacturer of choice and request a Cloudrail.OS container license here.\nOrder a Snowcone device as per the steps listed here (Job type: Local compute and storage only).\nDownload Snowcone device credentials \xe2\x80\x98unlock code\xe2\x80\x99 and \xe2\x80\x98manifest file\xe2\x80\x99 as described here.\nDownload AWS Opshub on the local machine used to interact with AWS Snowcone device via GUI.\nDownload SnowballEdge Client on the local machine used to interact with AWS Snowcone device via CLI.\nConfigure SnowballEdge Client by navigating here.\nII. Snowcone configration\nPower on the Snowcone device and connect it to local network device via Ethernet connection or Wifi (Router/Switch).\nConfigure RJ451 or RJ452 as DHCP/Static to get local LAN IP address on the Snowcone\xe2\x80\x99s display screen.\nUnlock Snowcone using AWS Opshub or SnowballEdge Client.\nLaunch the EC2 instance on the Snow device following the steps provided here. In this blog we will be using default Amazon Linux AMI validated to be used on Snow devices.\nFigure 2: Launch the EC2 instance using AWS Opshub for Snow\nCreate a direct network interface (DNI) and attach it to the Amazon EC2 instance as per the steps explained here.\nNote: DNI is only supported on RJ45 interface. DNI is required for the communication between IO-Link master and CloudRail.OS running on the EC2 instance.\nFigure 3: SnowconeEdge CLI used to set up a Direct Network Interface (DNI)\n III. CloudRail.OS set up \nSSH into EC2 instance\nssh -i <key-pair.pem> ec2-user@x.x.x.x\nsudo yum update -y\nInstall Docker\n$ sudo amazon-linux-extras install docker\n$ sudo service docker start\n$ sudo systemctl enable docker\n$ sudo usermod -a -G docker ec2-user\nPull the latest container image from docker public repository. Steps to set up container is found here. Latest CloudRail-image is found here.\nFor example.\n$ sudo docker pull cloudrailos/cr-container-os:beta-2.0.6\nThe \xe2\x80\x98cr-container-for-snow.zip\xe2\x80\x99 will contain module-credentials to be used by the container to connect to CloudRail DMC. Configure interface (to be used as field port for IO-Link master connectivity) in the container-config.json.\nFor example.\n$ sudo docker run -d \xe2\x80\x94name cr-firmware \\\n\xe2\x80\x94net=host -v '/home/ec2-user/cr-container-for-snow/cr-agent/cr-container':/home/cr-container \\\ncloudrailos/cr-container-os:beta-2.0.6\nIV. CloudRail management console registration\nLogin to CloudRail management console and register the serial number provided by CloudRail.\nOnce the box is added the status of the box should be \xe2\x80\x9conline\xe2\x80\x9d. Follow the steps here to set up CloudRail environment.\nBelow is the example of CloudRail console\nFigure 4: CloudRail console with Snowcone gateway appliance\nV. Processing the telemetry data\nIn order to set up CloudRail.OS to forward telemetry data to AWS IoT SiteWise follow these steps.\n"""
4,Ingest industrial data at scale with AWS IoT SiteWise Edge on Microsoft Windows Server,b'Costin B\xc4\x83dici',2023-04-17T17:33:20+00:00,https://d2908q01vomqb2.cloudfront.net/f6e1126cedebf23e1463aee73f9df08783640400/2023/04/04/Ingest-industrial-data-at-scale-with-AWS-IoT-SiteWise-Edge-on-Microsoft-Windows-Server.jpg-1024x576.png,https://aws.amazon.com/blogs/iot/ingest-industrial-data-at-scale-with-aws-iot-sitewise-edge-on-microsoft-windows-server/,"b'Introduction\nIndustrial customers in manufacturing, oil and gas, energy, and utilities industries commonly use Windows-based industrial devices and infrastructure. The recent release of AWS IoT SiteWise Edge for Windows, enables these customers to rapidly deploy and connect AWS IoT industrial facilities to AWS.\nCustomers will be able to install AWS IoT SiteWise Edge natively on Microsoft Windows without the need for any virtualization. This capability not only reduces the time to deploy AWS IoT SiteWise Edge, but also reduces costs and maintenance overhead by not having to acquire new infrastructure to run Linux.\nIn this blog post, we will walk through the installation and configuration of AWS IoT SiteWise Edge software on Windows to ingest equipment data to AWS IoT SiteWise. Subsequently, we discuss organizing and processing of ingested data in asset models. Finally, we will discuss configuring AWS IoT SiteWise Monitor to monitor equipment performance.\nSolution walk through\nThere are three key steps to consider when building this solution:\nAWS IoT SiteWise provides a number of components which deliver the capabilities shown in each step of the image above.\nSolution components\nStep 1: Collect data\nAWS IoT SiteWise gateway: This ingests data from IoT devices in the facility and transmits it securely to AWS IoT SiteWise. It supports connectivity to the devices using the standard OPC-UA, Modbus TCP, MQTT and Ethernet IP protocol.\nAWS IoT SiteWise Edge: For facilities with intermittent or unstable internet connection, AWS IoT SiteWise Edge brings the functionality of AWS IoT SiteWise and AWS IoT SiteWise Monitor to a local facility. It pulls in the configurations made in the cloud to a local device, allowing users to monitor asset performance locally even when there is a loss of internet connection.\nAWS IoT SiteWise Edge is a software package available on AWS IoT Greengrass that we will run on an existing Windows industrial gateway device at the manufacturing facility.\nStep 2: Organize and process data\nAWS IoT SiteWise: Once ingested, we need to organize and process the data collected. AWS IoT SiteWise allows us to add context and structure to the data to make it useful information. For example, what machine is the data coming from, how does this relate to the rest of the machines/processes in the facility, and how to define alarm limits on a metric.\nStep 3: Monitor equipment performance\nAWS IoT SiteWise Monitor: A no code web UI real-time monitoring portal, which allows users to monitor the performance of their equipment using information processed through AWS IoT SiteWise. It provides customizable dashboards and integrates with AWS Identity Access Management (IAM) to support authentication and authorization of users. It provides the capability to control which users can edit the dashboards and how much data a reader is allowed to access.\nIn this blog, we will focus on Step 1, particularly around ingesting data using AWS IoT SiteWise gateway. In this scenario, AWS IoT SiteWise gateway will be installed on a local industrial gateway device, running a Windows operating system. (Version 2016 or later).\nMetadata\n Time to read   20 minutes\n Learning level   300\n Services used   AWS IoT Greengrass V2, AWS IoT SiteWise, AWS IoT SiteWise Edge and AWS IoT SiteWise Monitor\nPrerequisites\nAt a minimum, AWS IoT SiteWise Edge requires an industrial computer running Linux or Windows with a x86 64 bit quad-core processor, 32GB RAM, and 256GB in disk space. The gateway device must allow inbound traffic on port 443 and it must allow outbound traffic on ports 443 and 8883.\nWe recommend you use dedicated hardware for AWS IoT SiteWise Edge. AWS IoT SiteWise Edge software runs on AWS IoT Greengrass V2 installed on Windows 2016 or later.\nSteps\nThese sections summarize how to create a SiteWise Edge gateway and includes detailed instructions for steps that are specific to creating a Windows gateway.\n1. Collect data\nCreate the gateway\nIn the AWS Management Console, create the SiteWise gateway, following Steps 1-4. During this process, choose Default setup and include a Data processing pack to enable edge capabilities. You can optionally choose to enable LDAP access, configure how data is published from the gateway to the cloud, and set up data sources.\nDuring Step 5 of the creation process, scroll down to the bottom of the page and select Windows as the industrial gateway OS. Click Generate.\nWhen the SiteWise Edge installer dialog box appears, click Acknowledge. Save the installer to a secure location, as you will not have access to it again.\nThe installer package (.ps1 extension) will be created and begin downloading to your local machine. In the AWS console, the gateway summary page will load. The Gateway configuration section of the Overview tab will show connectivity is \xe2\x80\x9cPending device pairing.\xe2\x80\x9d\nInstall edge software onto your industrial gateway\nOn the Windows server where you\xe2\x80\x99ll create the industrial gateway, log in as an administrator. If the edge installer was not downloaded directly to this machine, copy it to this machine.\nOnce the .ps1 file is on the Windows server, type PowerShell in the Windows search bar. In the search results, right-click on Windows PowerShell to bring up the context menu, and choose Run as administrator.\nAt the PowerShell prompt, change to the directory where you downloaded the installer file.\nRun the command below to unblock the installer file (replace Gateway-wAgJBvXS9.deploy.ps1 with the name of your file):\nunblock-file Gateway-wAgJBvXS9.deploy.ps1\nPowerShell\nThen, run the installer:\n./Gateway-wAgJBvXS9.deploy.ps1\nPowerShell\nWhen the installer finishes successfully, the message will say Done!\nThe SiteWise Edge gateway is now installed and running on the Windows server. To confirm, return to the gateway summary page in the AWS console (Services->AWS IoT SiteWise->Edge->Gateways-> select your gateway).\nThe Gateway configuration section of the Overview tab should now say \xe2\x80\x9cConnected.\xe2\x80\x9d\nSiteWise Edge gateways use packs to determine how to collect and store data. We recommend updating all packs installed on your gateway to their latest versions.\nThe rest of the process to create a SiteWise Edge gateway is the same for all gateway OS.\nConfigure your edge gateway from the Cloud\nOnce the gateway is created and connected, you can add an OPC-UA data source. As part of this process, provide the data source local endpoint (the hostname and port of the OPC-UA server), and select AWS IoT SiteWise as the data destination. If needed, the Advanced configuration options also allow you to specify authentication and other security settings.\nAfter adding a data source, the Data sources section in the Overview tab of the gateway summary page should show the source as \xe2\x80\x9cIn Sync\xe2\x80\x9d within 3-5 minutes. This confirms that data is being published from the Windows gateway to AWS IoT SiteWise.\n2. Organize and process data\nOnce data is being ingested into AWS IoT SiteWise, structure and enrich it with additional context. AWS IoT SiteWise Models and Assets enable you to create a virtual representation of your industrial operations. You can map industrial data streams to asset properties, create asset hierarchies, apply mathematical transformations to measurements, and create metrics for assets or groups of assets. In the AWS console, first create an asset model and then create assets.\n3. Monitor equipment performance\nOnce an AWS IoT SiteWise asset model is established, you can use AWS IoT SiteWise to monitor your equipment using alarms and web portals. To use alarms, first define IoT Events alarms on your asset models, and then configure alarm thresholds and notifications on specific assets.\nYou can also create web portals to monitor the data from your processes, devices, and equipment. In the console, create a AWS IoT SiteWise Monitor web portal to view and securely share operational dashboards. To use the web portal with limited network connectivity, enable your portal at the edge.\nCleaning Up\nBe sure to clean up the work in this blog to avoid charges. Delete the following resources when finished in this order.\nAWS IoT SiteWise Monitor\nDashboard\nProject\nPortal\nAWS IoT SiteWise Gateway, Asset, and Asset Model\nAWS IoT Greengrass Core device under menu Greengrass \xe2\x86\x92 Core devices\nCore Device Thing in AWS IoT Core under the menu Manage \xe2\x86\x92 Things\n'"
5,Introducing the latest AWS Well- Architected IoT Lens,b'Costin B\xc4\x83dici',2023-04-17T17:33:20+00:00,https://d2908q01vomqb2.cloudfront.net/f6e1126cedebf23e1463aee73f9df08783640400/2023/03/31/Introducing-the-latest-AWS-Well-Architected-IoT-Lens.jpg-1024x576.png,https://aws.amazon.com/blogs/iot/introducing-the-latest-aws-well-architected-iot-lens/,"b'Introduction \nWe are pleased to introduce the latest version of AWS Well-Architected IoT Lens. IoT projects can be complex due to a combination of many factors, including devices, software, use case scenarios, environments, processing patterns, network connectivity technologies, communication protocols, security issues, technical risks, compliance requirements and standards. The AWS Well-Architected IoT Lens provides simple and detailed guidance when building IoT workloads on AWS.\nSince 2015, the AWS Well-Architected Framework (WAF) has been helping AWS customers and partners improve their cloud architectures and reduce their technical risk. The framework consists of questions, design principles, and best practices across the six pillars: Operational Excellence, Security, Reliability, Performance Efficiency, Cost Optimization, and Sustainability.\nThe AWS Well-Architected Framework helps you understand the pros and cons of the design decisions you make when building systems on AWS. Using WAF, you can learn architectural best practices for designing and operating reliable, secure, efficient, and cost-effective systems in the cloud. The framework provides a way for you to consistently measure your architecture against industry best practices, and help you identify potential areas for improvement. We believe that having well-architected systems greatly increases the likelihood of business success.\nIn 2019, we introduced AWS Well-Architected Lens for IoT workloads.  The IoT Lens provides you with a consistent approach to evaluate your IoT architectures, implement scalable designs, and identify and mitigate technical risks. The IoT Lens covers common IoT implementation scenarios and identifies key workload elements to allow you to architect your IoT device, edge and cloud-based applications and workloads according to the best practices that we have gathered from supporting thousands of customer implementations.\nToday, we are delighted to introduce the latest version of the AWS Well-Architected Internet of Things (IoT) Lens white paper. Here\xe2\x80\x99s an overview of what\xe2\x80\x99s new in the IoT Lens.\nWhat\xe2\x80\x99s new in the IoT Lens?\nImproved best practices and implementation guidance \xe2\x80\x94 Notably, it provides you with actionable guidance that you can use to improve your workloads in the areas that matter most to your business.\nUpdated guidance on new features and services \xe2\x80\x94 AWS IoT continues to evolve with new services, features, and emerging best practices. A set of updated IoT features and services announced to-date have been incorporated into the IoT Lens whitepaper to help you create more well-architected workloads. Some of the new services and features include: AWS IoT ExpressLink, AWS IoT FleetWise, AWS IoT TwinMaker, AWS IoT SiteWise Edge, AWS IoT RoboRunner, and Fleet Hub for AWS IoT Device Management. These and other improvements will make it easier for your development team to create innovative IoT workloads in your enterprise.\nUpdated architectures and links \xe2\x80\x94 Many new documents, blogs, instructional and video links have been provided to reflect a host of new products, features, and current industry best practices to assist with your IoT projects.\nIndustrial Internet of Things (IIoT) \xe2\x80\x93 New Lens guidance has been provided for industrial IoT, manufacturing, and critical infrastructure. Although general best practices still apply, there are some additional considerations that you should consider to be put into place to support the greater criticality and larger impact of Operational Technology (OT) and IIoT systems. These issues are outlined in detail in the new Lens.\nUpdated Well-Architected questions \xe2\x80\x93 combining the IoT Lens Checklist and the questions in the original IoT Lens.\nWho should use the IoT Lens?\nThe IoT Lens will be useful for many roles, including:\nBusiness Leaders \xe2\x80\x94 to widen your appreciation of the end-to-end implementation and benefits of IoT\nChief Technology Officers \xe2\x80\x94 to understand how to use the new AWS services and implementation guidance\nIoT solution architects \xe2\x80\x94 to learn to build solutions according to the tenets of the Well-Architected Framework\nIoT embedded engineers \xe2\x80\x94 to design IoT devices according to the tenets of the Well-Architected Framework\nOperations team \xe2\x80\x94 to monitor and manage innovative infrastructures that drive the operation of a broader range of IoT workloads\nSecurity team members \xe2\x80\x93 to incorporate security requirements in their IoT projects\n'"
6,Guidance on using ISA/IEC 62443 for IIoT projects,b'Costin B\xc4\x83dici',2023-04-17T17:33:20+00:00,https://d2908q01vomqb2.cloudfront.net/f6e1126cedebf23e1463aee73f9df08783640400/2023/03/24/Guidance-on-using-ISA-IEC-62443-for-IIoT-projects.jpg-1024x576.png,https://aws.amazon.com/blogs/iot/guidance-on-using-isa-iec-62443-for-iiot-projects/,"b'Introduction\nWith the increasing proliferation of Industrial Internet of Things (IIoT) systems and cloud services for innovation and digital transformation, government agencies and industrial customers are faced with protecting an expanding attack surface. The ISA/IEC 62443 series of standards were written before IIoT technologies were common but provide a strong basis for securing these environments. In this blog, we discuss the ISA/IEC 62443 standards, what is changing in the standards, and certifications to support the use of IIoT in Industrial Automation and Control Systems (IACS).\nBackground    \nThe ISA/IEC 62443 series of standards are developed jointly by ISA99 and IEC to address the need to design cybersecurity robustness and resilience into IACS. The goal in applying the 62443 series is to improve the safety, availability, integrity and confidentiality of components or systems used for industrial automation and control. In addition, they provide criteria for procuring and implementing secure industrial automation and control systems. Conformance with the requirements of the 62443 series is intended to improve cyber security and help identify and address vulnerabilities, reducing the risk of compromising confidential information or causing degradation or failure of the equipment (hardware and software) of processes under control. The 62443 series builds on established standards for the security of general-purpose information technology (IT) systems (e.g., the ISO/IEC 27000 series), identifying and addressing the important differences present in IACS. Many of these differences are based on the reality that cyber security risks with IACS may have Health, Safety, or Environment (HSE) implications and the response should be integrated with other existing risk management practices.\nISA/IEC 62443 is \xe2\x80\x9cconsensus-based,\xe2\x80\x9d comprehensive, and widely used across industries. Today, the growing availability of IIoT has widened the array of technologies and methodologies available for use in industrial automation environments. This growth increases the attack surface, which inherently increases the risk of compromise in these environments. To secure environments that use IIoT in IACS, a thorough understanding of IACS cybersecurity lifecycle is beneficial. The ISA/IEC 62443 series can provide a risk-based, defense-in-depth, and performance-based approach that can assist asset owners and their service providers in navigating the use of IIoT in industrial automation and control systems.\nUnderstanding the ISA/IEC 62443 Standards\nISA/IEC 62443, officially ANSI/ISA/IEC 62443, is a set of standards and technical reports that deal with industrial cybersecurity. Holistically, ISA/IEC 62443 is designed to help asset owners (end users), system integrators, and manufacturers reduce the risk of deploying and operating an IACS. Figure 1 gives an idea of the different parts of the standard. You can see that it is a multi-part standard.\nFigure 1: ISA/IEC 62443 documents (Courtesy of ISA)\nThese documents are arranged in four groups, corresponding to the primary focus and intended audience/role. It\xe2\x80\x99s helpful to consider the structure of these standards and how the hierarchy defines the roles and responsibilities for providing a robust IACS security posture.\nGeneral \xe2\x80\x93 This group includes documents that address topics that are common to the entire series.\nPolicies and Procedures \xe2\x80\x93 Documents in this group focus on the policies and procedures associated with IACS security.\nSystem Requirements \xe2\x80\x93 The documents in the third group address requirements at the system level.\nComponent Requirements \xe2\x80\x93 The fourth and final group includes documents that provide information about the more specific and detailed requirements associated with the development of IACS products.\nThe benefit of these standards is that asset owners can more easily (than on their own) define a required security level that references to a specific threat level, a measure that provides tighter security controls for higher risk functions. The benefit for service providers is that the standards provide clear explicit language of the requirements specified from the end user. And the benefit for product or component manufacturers is that they can more clearly describe the functionality of their products (from a security perspective) and differentiate themselves competitively, all of which is better than simply providing a long list of security features.\nPERA model and ISA TR 62443-4-3 (draft)\nToday, with the growing use of IIoT in Operational Technology (OT) environments, there is a need for the standards to be updated to support IIoT. Even though the standards were written before IIoT technologies were common, most concepts remain applicable or can be adapted for that environment. ISA 99 Working Group 9 published a Technical Report ISA TR 62443-4-3 (draft) which IEC calls IEC PAS 62443-4-3 (draft) which address the use of IIoT technology in IACS.\nPreviously, the Purdue Enterprise Reference Architecture (PERA) popularly referred to as the Purdue Model was used as a reference model for IACS. That model was rooted in several assumptions about technology and connections that IIoT technology can upset. With the advent of IIoT technology, the norms of the PERA model have been blurred as conventional thinking of physical network segregation and levels of functionality are changed by the internet-connected nature of IIoT technology.  IIoT technology has not rendered the model\xe2\x80\x99s illustration of functionality obsolescent but has blurred the network architecture analogy made during the 1990s on where these functionalities can reside. For example, in that model, the devices at Level 0 (the field level) were not as smart and had no connectivity directly to external systems. Today, however, a small temperature or vibration sensor can also be an IIoT device, that can connect to the cloud directly, bypassing all higher levels of the PERA model. The PERA model was used to describe functionality of existing IACS, but it began to be used as a model to implement a secured architecture, which was not originally envisaged.\nFigure 2: IIoT upsets the traditional Purdue (PERA) model (Adapted from ISA/IEC 62443-4-3 (draft))\nAssessing OT and IIoT cybersecurity risk, provides an example of zones and conduits in IACS with IIoT systems and discusses how asset owners can use ISA/IEC 62443-3-2, Security Risk Assessment for System Design. This is a key step in the risk assessment process by partitioning the System Under Consideration (SUC) into separate Zones and Conduits. The intent is to identify those assets which share common security characteristics in order to establish a set of common security requirements that reduce cybersecurity risk. Partitioning the SUC into Zones and Conduits can also reduce overall risk by limiting the impact of a cyber incident. Zone and conduit diagrams can assist in detailed IIoT cyber security risk assessments and help in identifying threats, and vulnerabilities, determining consequences and risks and providing remediations or control measures to safeguard assets from cyber events.\nThe draft Technical Report 62443-4-3 provides several examples of security capabilities which can be offered by Cloud Providers which asset owners can take advantage of for securing their IIoT solutions to achieve their security level targets. Refer to the table enclosed for a description of these security capabilities and AWS resources available to asset owners:\nIIoT cloud-based functionality (CBF) Security Controls Explanation\nIdentity management\nCloud providers can provide identity management capabilities for IIoT. These capabilities can include both the management of identity for devices as well as authentication and authorization for user access.\nEXAMPLE: The cloud service provider can support the use of hardware security modules (HSM), rotation of credentials.\nAWS resources\nAWS provides the following assets and services to help with identity management:\nSecurity and Identity for AWS IoT\nAmazon Cognito is a service that provides authentication, authorization, and user management for your web and mobile apps.\nAWS Identity and Access Management (IAM) is a service that enables you to manage access to AWS services and resources securely.\nDevice authentication and authorization for AWS IoT Greengrass.\nAWS Secrets Manager is a service that can be used to securely store and manage secrets in the cloud and encrypts the secrets using AWS KMS.\nIdentifying IoT device certificates with a revoked intermediate CA blog\nHow to manage IoT device certificate rotation with AWS IoT blog\nEnhancing IoT device security using HSM and AWS IoT Device SDK blog\nAuthorization management for components\nCloud providers can provide rights management capabilities to control access and authorization within the cloud and, in some cases, to IIoT CBF equipment.\nAWS resources\nAWS provides the following assets and services to help with authorization management for components:\nSecurity and Identity for AWS IoT\nAmazon Cognito is a service that provides authentication, authorization, and user management for your web and mobile apps.\nAWS Identity and Access Management (IAM) is a service that enables you to manage access to AWS services and resources securely.\nDevice authentication and authorization for AWS IoT Greengrass.\nAWS IoT Core Authorization\nData protection policies Cloud providers can provide capabilities to assist asset owners in protecting data availability, integrity, privacy and confidentiality in IIoT CBF including use of encryption for data in transit and at rest.\nEXAMPLE: Supporting asset owner\xe2\x80\x99s data classification and safeguardingAWS resourcesAWS provides the following assets and services to help with data protection:\nAWS Shared Responsibility Model for security and compliance.\nAWS Data Privacy\nAWS Compliance Programs and Offerings\nAWS Compliance Solutions Guide\nAWS KMS enables you to easily create and control the keys used for cryptographic operations in the cloud.\nData protection in AWS IoT SiteWise\nAmazon Macie to discover and protect sensitive IIoT data at scale.\nPrivacy Features of AWS Services\nData residency policies\nCloud providers can provide the capability for asset owners to establish residency controls for data in the cloud.\nAWS resources\nAWS provides the following assets and services to help with data residency requirements:\nAWS Global Infrastructure\nAWS Data Residency whitepaper\nAddressing Data Residency with AWS blog\nAWS Outposts allows you to extend and run native AWS services on premises\nAWS Hybrid Cloud services extends AWS infrastructure and services to on premises and at the edge\nSecure communications management\nCloud providers can offer services such as VPNs or other secure communication capabilities for IIoT CBF communications. These capabilities can include a service to convert insecure automation protocols into secure communication protocols before transmission.\nAWS resources\nAWS provides the following assets and services to help with secure communications management:\nAWS IoT SDKs to help you securely and quickly connect devices to AWS IoT.\nFreeRTOS Libraries for networking and security in embedded applications.\nSecurity best practices for AWS IoT SiteWise\n AWS Virtual Private Network (VPN) solutions establish secure connections between industrial plants and AWS global network.\nAWS Direct Connect is a cloud service solution that makes it easy to establish a dedicated network connection from your premises to AWS.\nAWS IoT SiteWise gateway allow you to ingest data using industrial protocols such as OPC-UA, Modbus TCP and Ethernet/IP, etc.\n Machine to Cloud Connectivity Framework\nAudit and monitoring services\nCloud providers can offer audit and monitoring capabilities for IIoT CBF, including the ability to centrally log events and provide analysis. This can also include threat detection and behavior anomalies.\nAWS resources\nAWS provides the following assets and services to help with audit and monitoring:\nAWS IoT Device Defender to monitor and audit your fleet of IoT devices.\nMonitoring AWS IoT with CloudWatch Logs to centralize the logs from all of your systems, applications, and AWS services that you use, in a single, highly scalable service.\nLogging AWS IoT API Calls with AWS CloudTrail to provide a record of actions taken by a user, a role, or an AWS service in AWS IoT.\nMonitoring with AWS IoT Greengrass logs\nAWS Config to assess, audit, and evaluate the configurations of your AWS resources.\nAmazon GuardDuty to continuously monitor for malicious activity and unauthorized behavior to protect your AWS accounts and workloads.\nAWS Security Hub to automate AWS security checks and centralize security alerts.\nImplement security monitoring across OT, IIoT and cloud blog\nIncident response\nCloud providers can provide capabilities to supplement asset owner\xe2\x80\x99s incident response activities\nAWS resources\nAWS provides the following assets and services to help with incident response:\nAWS Security Incident Response Guide\n AWS Systems Manager provides a centralized and consistent way to gather operational insights and carry out routine management tasks.\n Enable compliance and mitigate IoT risks with automated incident response blog\nAWS Incident response blogs\nAWS Customer Incident Response Team blog\nPatch management\nCloud providers can provide patching capabilities for IIoT CBF equipment.\nAWS resources\nAWS provides the following assets and services to help with patch management:\nFreeRTOS Over-the-Air Updates\nAWS IoT Greengrass Core Software OTA Updates\nAWS IoT jobs to define a set of remote operations that you send to and execute on one or more devices connected to AWS IoT.\nAWS Systems Manager Patch Manager automates the process of patching managed instances with both security related and other types of updates such as operating systems and applications.\nSchedule remote operations using AWS IoT Device Management Jobs blog\nSecurity analytics\nCloud providers can provide the capability to identify anomalies to gain insights on complex events which can be used to improve the security posture of your IIoT Cloud Based Functionality (CBF). This can enable the asset owner to detect and respond to incidents in a timely manner.\nAWS resources\nAWS provides the following assets and services to help with security analytics:\nAWS IoT Device Defender helps you identify and respond to IoT security issues\n AWS IoT Events helps you detect and respond to events from IoT sensors and applications\nAmazon GuardDuty protects your AWS accounts with intelligent threat detection\n Amazon Security Lake helps you centralize security data for analytics\n AWS services for security analytics\nBackup and Recovery of OT and IIoT data\nCloud providers can provide backup and recovery options for IIoT CBF data.\nAWS resources\nAWS provides the following assets and services to help with backup and recovery of OT and IIoT data:\n Resilience in AWS IoT Greengrass to help support data resiliency and backup needs.\n Backup and Restore Use Cases with AWS\nCloudEndure Disaster Recovery for fast and reliable recovery into AWS.\nAWS Backup to centrally manage and automate backups across AWS services.\nDisaster Recovery for AWS IoT solution guidance\nFigure 3: Examples of security capabilities offered by cloud providers (from TR-62443-4-3) along with AWS services and guidance.\nOther useful AWS resources for asset owners include the AWS Well Architected Framework, IoT Lens to design, deploy, and architect IIoT workloads aligned with architectural best practices and AWS Security Best Practices for Manufacturing OT whitepaper.\nISASecure IIoT Component Security Assurance (ICSA)\nThe ISASecure program announced a new ISASecure certification for Industrial Internet of Things (IIoT) components based on the ISA/IEC 62443 series of standards. The certification addresses the need for industry-vetted IIoT certification program. The ISASecure IIoT Component Security Assurance (ICSA) is a security certification program for IIoT devices and IIoT gateways. ICSA is based upon the 62443 standard and a component that meets the requirements of the ISASecure ICSA specification will earn the ISASecure ICSA certification; a trademarked designation that provides recognition of product security characteristics and capabilities, and provides an independent industry stamp of approval similar to a \xe2\x80\x98Safety Integrity Level\xe2\x80\x99 Certification (ISO/IEC 61508). The ICSA is based on 62443-4-1 and 62443-4-2 with some exceptions and extensions. The extensions clarify the application of 62443 principles to IIoT environments. Examples are creating \xe2\x80\x9cinternal\xe2\x80\x9d zones using compartmentalization technologies, controlling application of software updates, securing remote management, device authentication strength, and component resilience to cloud services or the cloud interface. In addition, an ongoing security maintenance audit is required to maintain certification. Cloud services are not in scope for this certification.\n'"
7,How to build smart applications using Protocol Buffers with AWS IoT Core,b'Costin B\xc4\x83dici',2023-04-17T17:33:20+00:00,https://d2908q01vomqb2.cloudfront.net/f6e1126cedebf23e1463aee73f9df08783640400/2023/03/21/How-to-build-smart-applications-using-Protocol-Buffers-with-AWS-IoT-Core-1024x576.png,https://aws.amazon.com/blogs/iot/how-to-build-smart-applications-using-protocol-buffers-with-aws-iot-core/,"b'Introduction to Protocol Buffers\nProtocol Buffers, or Protobuf, provide a platform-neutral approach for serializing structured data. Protobuf is similar to JSON, except it is smaller, faster, and is capable of automatically generating bindings in your preferred programming language.\nAWS IoT Core is a managed service that lets you connect billions of IoT devices and route trillions of messages to AWS services, enabling you to scale your application to millions of devices seamlessly. With AWS IoT Core and Protobuf integration, you can also benefit from Protobuf\xe2\x80\x99s lean data serialization protocol and automated code binding generation.\nAgility and security in IoT with Protobuf code generation\nA key advantage comes from the ease and security of software development using Protobuf\xe2\x80\x99s code generator. You can write a schema to describe messages exchanged between the components of your application. A code generator (protoc or others) interprets the schema and implements the encoding and decoding function in your programming language of choice. Protobuf\xe2\x80\x99s code generators are well maintained and widely used, resulting in robust, battle-tested code.\nAutomated code generation frees developers from writing the encoding and decoding functions, and ensures its compatibility between programming languages. Allied with the new launch of AWS IoT Core\xe2\x80\x99s Rule Engine support for Protocol Buffer messaging format, you can have a producer application written in C running on your device, and an AWS Lambda function consumer written in Python, all using generated bindings.\nOther advantages of using Protocol Buffers over JSON with AWS IoT Core are:\nSchema and validation: The schema is enforced both by the sender and receiver, ensuring that proper integration is achieved. Since messages are encoded and decoded by the auto-generated code, bugs are eliminated.\nAdaptability: The schema is mutable and it is possible to change message content maintaining backward and forward compatibility.\nBandwidth optimization: For the same content, message length is smaller using Protobuf, since you are not sending headers, only data. Over time this provides better device autonomy and less bandwidth usage. A recent research on Messaging Protocols and Serialization Formats revealed that a Protobuf formatted message can be up to 10 times smaller than its equivalent JSON formatted message. This means fewer bytes effectively go through the wire to transmit the same content.\nEfficient decoding: Decoding Protobuf messages is more efficient than decoding JSON, which means recipient functions run in less time. A benchmark run by Auth0 revealed that Protobuf can be up to 6 times more performant than JSON for equivalent message payloads.\nThis blog post will walk you through deploying a sample application that publishes messages to AWS IoT Core using Protobuf format. The messages are then selectively filtered by the AWS IoT Core Rules Engine rule.\nLet\xe2\x80\x99s review some of the basics of Protobuf.\nProtocol Buffers in a nutshell\nThe message schema is a key element of Protobuf. A schema may look like this:\nsyntax = ""proto3"";\n\nimport ""google/protobuf/timestamp.proto"";\n\nmessage Telemetry\n{\n  enum MsgType\n  {\n    MSGTYPE_NORMAL = 0;\n    MSGTYPE_ALERT = 1;\n  }\n  MsgType msgType = 1;\n  string instrumentTag = 2;\n  google.protobuf.Timestamp timestamp = 3;\n  double value = 4;\n}\nThe first line of the schema defines the version of Protocol Buffers you are using. This post will use proto3 version syntax, but proto2 is also supported.\nThe following line indicates that a new message definition called Telemetry will be described.\nThis message in particular has four distinct fields:\nA msgType field, which is of type MsgType and can only take on enumerated values ""MSGTYPE_NORMAL"" or ""MSGTYPE_ALERT""\nAn instrumentTag field, which is of type string and identifies the measuring instrument sending telemetry data\nA timestamp field of type google.protobuf.Timestamp which indicates the time of the measurement\nA value field of type double which contains the value measured\nPlease consult the complete documentation for all possible data types and additional information on the syntax.\nA Telemetry message written in JSON looks like this:\n{\n  ""msgType"": ""MSGTYPE_ALERT"",\n  ""instrumentTag"": ""Temperature-001"",\n  ""timestamp"": 1676059669,\n  ""value"": 72.5\n}\nJSON\nThe same message using protocol Buffers (encoded as base64 for display purposes) looks like this:\n0801120F54656D70657261747572652D3030311A060895C89A9F06210000000000205240\nNote that the JSON representation of the message is 115 bytes, versus the Protobuf one at only 36 bytes.\nOnce the schema is defined protoc can be used to:\nCreate bindings in your programming language of choice\nCreate a FileDescriptorSet, that is used by AWS IoT Core to decode received messages.\nUsing Protocol Buffers with AWS IoT Core\nProtobuf can be used in multiple ways with AWS IoT Core. The simplest way is to publish the message as binary payload and have recipient applications decode it. This is already supported by AWS IoT Core Rules Engine and works for any binary payload, not just Protobuf.\nHowever, you get the most value when you want to decode Protobuf messages for filtering and forwarding. Filtered messages can be forwarded as Protobuf, or even decoded to JSON for compatibility with applications that only understand this format.\nThe recently launched AWS IoT Rules Engine support for Protocol Buffer messaging format allows you to do just that with minimal effort, in a managed way. In the following sections we will guide you through deploying and running a sample application.\nPrerequisites\nTo run this sample application you must have the following:\nA computer with protoc installed. Refer to the official installation instructions for your Operating System.\nAWS CLI (installation instructions)\nAWS account and valid credentials with full permissions on Amazon S3, AWS IAM, AWS IoT Core and AWS CloudFormation\nPython 3.7 or newer\nSample application: Filtering and forwarding Protobuf messages as JSON\nTo deploy and run the sample application, we will perform 7 simple steps:\nDownload the sample code and install Python requirements\nConfigure your IOT_ENDPOINT and AWS_REGION environment variables\nUse protoc to generate Python bindings and message descriptors\nRun a simulated device using Python and the Protobuf generated code bindings\nCreate AWS Resources using AWS CloudFormation and upload the Protobuf file descriptor\nInspect the AWS IoT Rule that matches, filters and republishes Protobuf messages as JSON\nVerify transformed messages are being republished\nStep 1: Download the sample code and install Python requirements\nTo run the sample application, you need to download the code and install its dependencies:\nFirst, download and extract the sample application from our AWS github repository: https://github.com/aws-samples/aws-iotcore-protobuf-sample\nIf you downloaded it as a ZIP file, extract it\nTo install the necessary python requirements, run the following command within the folder of the extracted sample application\npip install -r requirements.txt\nThe command above will install two required Python dependencies: boto3 (the AWS SDK for Python) and protobuf.\nStep 2: Configure your IOT_ENDPOINT and AWS_REGION environment variables\nOur simulated IoT device will connect to the AWS IoT Core endpoint to send Protobuf formatted messages.\nIf you are running Linux or Mac, run the following command. Make sure to replace <AWS_REGION> with the AWS Region of your choice.\nexport AWS_REGION=<AWS_REGION>\nexport IOT_ENDPOINT=$(aws iot describe-endpoint --endpoint-type iot:Data-ATS --query endpointAddress --region $AWS_REGION --output text)\nStep 3: Use protoc to generate Python bindings and message descriptor\nThe extracted sample application contains a file named msg.proto similar to the schema example we presented earlier.\nRun the commands below to generate the code bindings your simulated device will use to generate the file descriptor.\nprotoc --python_out=. msg.proto\nprotoc -o filedescriptor.desc msg.proto\nAfter running these commands, you should see in your current folder two new files:\nfiledescriptor.desc msg_pb2.py\nStep 4: Run the simulated device using Python and the Protobuf generated code bindings\nThe extracted sample application contains a file named simulate_device.py.\nTo start a simulated device, run the following command:\npython3 simulate_device.py\nVerify that messages are being sent to AWS IoT Core using the MQTT Test Client on the AWS console.\nAccess the AWS IoT Core service console: https://console.aws.amazon.com/iot; make sure you are in the correct AWS Region.\nUnder Test, select MQTT test client.\nUnder the Topic filter, fill in test/telemetry_all\nExpand the Additional configuration section and under MQTT payload display select Display raw payloads.\nClick Subscribe and watch as Protobuf formatted messages arrive into the AWS IoT Core MQTT broker.\nStep 5: Create AWS Resources using AWS CloudFormation and upload the Protobuf file descriptor\nThe extracted sample application contains an AWS CloudFormation template named support-infrastructure-template.yaml.\nThis template defines an Amazon S3 Bucket, an AWS IAM Role and an AWS IoT Rule.\nRun the following command to deploy the CloudFormation template to your AWS account. Make sure to replace <YOUR_BUCKET_NAME> and <AWS_REGION> with a unique name for your S3 Bucket and the AWS Region of your choice.\naws cloudformation create-stack --stack-name IotBlogPostSample \\\n--template-body file://support-infrastructure-template.yaml \\\n--capabilities CAPABILITY_IAM \\\n--parameters ParameterKey=FileDescriptorBucketName,ParameterValue=<YOUR_BUCKET_NAME> \\\n--region=<AWS_REGION>\nAWS IoT Core\xe2\x80\x99s support for Protobuf formatted messages requires the file descriptor we generated with protoc. To make it available we will upload it to the created S3 bucket. Run the following command to upload the file descriptor. Make sure to replace <YOUR_BUCKET_NAME> with the same name you chose when deploying the CloudFormation template. aws s3 cp filedescriptor.desc s3://<YOUR_BUCKET_NAME>/msg/filedescriptor.desc\nStep 6: Inspect the AWS IoT Rule that matches, filters, and republishes Protobuf messages as JSON\nLet\xe2\x80\x99s assume you want to filter messages that have a msgType of MSGTYPE_ALERT, because these indicate there might be dangerous operating conditions. The CloudFormation template creates an AWS IoT Rule that decodes the Protobuf formatted message our simulated device is sending to AWS IoT Core, it then selects those that are alerts and republishes, in JSON format, so that another MQTT topic responder can subscribe to. To inspect the AWS IoT Rule, perform the following steps:\nAccess the AWS IoT Core service console: https://console.aws.amazon.com/iot\nOn the left-side menu, under Message Routing, click Rules\nThe list will contain an AWS IoT Rule named ProtobufAlertRule, click to view the details\nUnder the SQL statement, note the SQL statement, we will go over the meaning of each element shortly\nUnder Actions, note the single action to Republish to AWS IoT topic\nSELECT\n  VALUE decode(encode(*, \'base64\'), ""proto"", ""<YOUR_BUCKET_NAME>"", ""msg/filedescriptor.desc"", ""msg"", ""Telemetry"")\nFROM\n  \'test/telemetry_all\'\nWHERE\n  decode(encode(*, \'base64\'), ""proto"", ""<YOUR_BUCKET_NAME>"", ""msg/filedescriptor.desc"", ""msg"", ""Telemetry"").msgType = \'MSGTYPE_ALERT\'\nSQL\nThis SQL statement does the following:\nThe SELECT VALUE decode(...) indicates that the entire decoded Protobuf payload will be republished to the destination AWS IoT topic as a JSON payload. If you wish to forward the message still in Protobuf format, you can replace this with a simple SELECT *\nThe WHERE decode(...).msgType = \'MSGTYPE_ALERT\' will decode the incoming Protobuf formatted message and only messages containing field msgType with value MSGTYPE_ALERT will be forwarded\nStep 7: Verify transformed messages are being republished\nIf you click on the single action present in this AWS IoT Rule, you will note that it republishes messages to the topic/telemetry_alerts topic.\nThe destination topic test/telemetry_alerts is part of the definition of the AWS IoT Rule action, available in the AWS CloudFormation template of the sample application.\nTo subscribe to the topic and see if JSON formatted messages are republished, follow these steps:\nAccess the AWS IoT Core service console: https://console.aws.amazon.com/iot\nUnder Test, select MQTT test client\nUnder the Topic filter, fill in test/telemetry_alerts\nExpand the Additional configuration section and under MQTT payload display make sure Auto-format JSON payloads option is selected\nClick Subscribe and watch as JSON-converted messages with msgType MSGTYPE_ALERT arrive\nIf you inspect the code of the simulated device, you will notice approximately 20% of the simulated messages are of MSGTYPE_ALERT type and messages are sent every 5 seconds. You may have to wait to see an alert message arrive.\nClean Up\nTo clean up after running this sample, run the commands below:\n# delete the file descriptor object from the Amazon S3 Bucket\naws s3 rm s3://<YOUR_BUCKET_NAME>/msg/filedescriptor.desc\n\n# detach all policies from the IoT service role\naws iam detach-role-policy --role-name IoTCoreServiceSampleRole \\\n  --policy-arn $(aws iam list-attached-role-policies --role-name IoTCoreServiceSampleRole --query \'AttachedPolicies[0].PolicyArn\' --output text)\n\n# delete the AWS CloudFormation Stack\naws cloudformation delete-stack --stack-name IotBlogPostSample\n'"
8,Building an OCPP-compliant electric vehicle charge point operator solution using AWS IoT Core,b'Costin B\xc4\x83dici',2023-04-17T17:33:20+00:00,https://d2908q01vomqb2.cloudfront.net/f6e1126cedebf23e1463aee73f9df08783640400/2023/03/21/Building-an-OCPP-compliant-electric-vehicle-charge-point-operator-solution-using-AWS-IoT-Core.jpg-1024x576.png,https://aws.amazon.com/blogs/iot/building-an-ocpp-compliant-electric-vehicle-charge-point-operator-solution-using-aws-iot-core/,"b'The shift from fossil fuels to electric powered vehicles is a key component of government and commercial commitments to achieve net-zero emissions by 2050. It is projected that the United States alone will require a national network of at least 500,000 electric vehicle (EV) chargers by 2030 to support the projected number of EVs on the road [1][2]. Globally, governments and industries are partnering to build millions of public charging and private fleet charging networks [3].\nInstalling and powering the physical charging infrastructure is just the first step \xe2\x80\x94 chargers (Charge Points or \xe2\x80\x9cCP\xe2\x80\x9d) need to be continuously monitored and managed by their operators (Charge Point Operators or \xe2\x80\x9cCPO\xe2\x80\x9d). CPOs are responsible for regular remote and on-site maintenance, collecting health metrics, and managing operational configurations. CPOs are also responsible for ensuring that the CPs are compatible with the latest industry standards and protocols, like Open Charge Point Protocol (OCPP) and ISO 15118. And all this must be implemented with security measures that can support CPs at scale.\nThis post demonstrates how AWS services like AWS IoT Core, Amazon Elastic Container Service (Amazon ECS), and AWS Lambda can be used to build a highly-scalable, low-latency electric vehicle charge point operator system based on the EV industry standard, OCPP.\nAbout AWS IoT Core\nAWS IoT Core lets you connect billions of devices and route trillions of messages to and from AWS services without managing infrastructure. AWS IoT Core handles the heavy-lifting of scaling and message routing\xe2\x80\x94making it easier for customers needing to support large fleets of remote devices, like CPs, communicating through publish-and-subscribe patterns. AWS IoT Core natively implements MQTT, HTTPS, and MQTT over WebSockets, and can be adapted to support other protocols, like OCPP.\nOverview\nMost commercially available CPs implement OCPP as a means of bi-directional publish-and-subscribe communication with a CPO. Operating a CPO on AWS requires the introduction of an OCPP WebSocket endpoint, with which CPs communicate. That endpoint, described here as the OCPP Gateway, acts as a proxy between OCPP and MQTT, enabling integration with AWS IoT Core and downstream CPO services built on AWS.\nThe following architecture diagram illustrates the high-level end-to-end solution you will build in this blog post.\nFigure 1: Charge Point OCPP message proxied to CPO Service via one-to-one relationship between WebSocket connection and MQTT topic\nArchitecture\nThe architecture diagram below depicts the resources that this solution will deploy into your account.\nFigure 2: OCPP Gateway solution stack architecture\nThe OCPP Gateway is deployed as an Amazon ECS application which can run on either AWS Fargate or Amazon Elastic Compute Cloud (EC2). AWS Fargate eliminates the need for infrastructure management and is the preferred option for this solution. Containerized applications can be scaled horizontally, allowing the OCPP Gateway to automatically scale up or down as the number of connected CPs changes. The long running nature of ECS tasks allows for WebSockets connections to be maintained for extended periods, reducing network traffic and connection overheads.\nA Network Load Balancer (NLB) fronts multiple OCPP Gateway containers. The NLB provides a single, fully qualified domain name (FQDN) that serves as the OCPP endpoint to which CPs initiate connection. Upon connection initiation, the NLB will route the charge point connection to one of the OCPP Gateway instances, which will establish the WebSocket connection between itself and the CP.\nWhen a CP establishes a socket connection with an instance of the OCPP Gateway, that Handler sets up an MQTT connection to AWS IoT Core using the CP\xe2\x80\x99s unique identifier as the Thing ID. That client subscribes to MQTT message topics associated with that CP.\nThe MQTT client implemented by the OCPP Gateway is socket aware, thereby providing a one-to-one association between the MQTT subscription and the CP. Any messages initiated by the CPO will be delivered to the MQTT client associated with the destination CP and forwarded over the socket to that CP. AWS IoT Core is highly elastic and will readily scale as more CPs are on-boarded.\nSolution walk-through\nThis solution demonstrates how you can use AWS to build a scalable CPO by deploying the OCPP Gateway to integrate with AWS IoT Core. The steps below will walk you through the deployment of an OCPP Gateway into your AWS account, will demonstrate how you can simulate CP message, and will provide examples of you how can act on those message using AWS resources.\nPrerequisites\nVerify that your environment satisfies the following prerequisites:\nYou have:\nAn AWS account\nAdministratorAccess policy granted to your AWS account (for production, we recommend restricting access as needed)\nBoth console and programmatic access\nAWS CLI installed and configured to use with your AWS account\nNodeJS 12+ installed\nTypescript 3.8+ installed\nAWS CDK CLI installed\nDocker installed\nPython 3+ installed\nPrepare the CDK\nThe solution will be deployed into your AWS account using infrastructure-as-code wih the AWS Cloud Development Kit (CDK).\nClone the repository:\ngit clone https://github.com/aws-samples/aws-ocpp-gateway.git\nBash\nNavigate to this project on your computer using your terminal:\ncd aws-ocpp-gateway\nBash\nInstall the project dependencies by running this command:\nnpm install\nBash\nSet environment variables for CDK to the target AWS account ID and region where you wish to deploy this stack\nNote: AWS IoT Core is available in these AWS regions.\nexport CDK_DEPLOY_ACCOUNT=targetAccountId (e.g. 12345678910)\nexport CDK_DEPLOY_REGION=targetRegion (e.g. eu-west-1)\nBash\n(Optional) Bootstrap AWS CDK on the target account and regioon\nNote: This is required if you have never used AWS CDK before on this account and region combination. (More information on CDK bootstrapping).\nnpx cdk bootstrap aws://{targetAccountId}/{targetRegion}\nBash\n(Optional) Enable WebSockets using TLS with your own domain name\nIf you have an Amazon Route 53 hosted zone in your account, this solution can automatically:\nCreate subdomain (A Record) gateway.yourdomain.com\nCreate an AWS Certificate Manager (ACM) SSL certificate for it\nEnable TLS for your gateway wss://gateway.yourdomain.com\nUncomment this line in /bin/aws-ocpp-gateway.ts and replace yourdomain.com with your own domain name (i.e. example.com)\n  // domainName: \'yourdomain.com\',\nBash\nDeploy the solution to your AWS Account\nVerify that Docker is running with the following command:\ndocker version\nBash\nNote: If you get an error like the one below, then Docker is not running and need to be restarted:\nCannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\nBash\nDeploy the OCPP Gateway using the following CDK command:\nnpx cdk deploy\nBash\nNote: This step can take about 10 minutes, depending on your computer and network speed.\nYou can view the progress of your CDK deployment in the CloudFormation console in the selected region.\nScreenshot: AWS CloudFormation stack resources\nOnce deployed, take note of the AwsOcppGatewayStack.websocketURL value\nNote: This WebSocket URL is the entry point that will be set in your CP configurations or in the\nEV Charge Point simulator described below.\nIf you used your own domain, your output will look like:\n..\nOutputs:\nAwsOcppGatewayStack.loadBalancerDnsName = gateway.example.com\n\xf0\x9f\x91\x89 AwsOcppGatewayStack.websocketURL = wss://gateway.example.com\n...\nBash\nOtherwise, like this:\n...\nOutputs:\nAwsOcppGatewayStack.loadBalancerDnsName = ocpp-gateway-xxxxxxx.elb.xx-xxxx-x.amazonaws.com\n\xf0\x9f\x91\x89 AwsOcppGatewayStack.websocketURL = ws://ocpp-gateway-xxxxxxx.elb.xx-xxxx-x.amazonaws.com\n...\nBash\nSimulating CP connectivity\nWe have provided the simulate.py Python script to help you test and explore the capability of the OCPP Gateway and AWS IoT Core without the need for a physical CP. Other OCPP simulators, like OCPP-2.0-CP-Simulator, can also be used.\nSimulation setup\nIn AWS Explorer, select your region and open AWS IoT Core, All devices, Things. On the Things tab choose Create a things.\nSelect Create single thing and choose Next\nEnter a Thing name\nNote: Each EV Charge Point must map to a single IoT Thing. For our test, we\xe2\x80\x99ll set the Thing name as CP1\nScreenshot: Creating an IoT Thing\nChoose Next\nFor Device certificate, select Skip creating a certificate at this time, and choose Create thing\nScreenshot: Skip the certification creation\nNavigate to this folder with your terminal:\ncd ev-charge-point-simulator\nBash\nCreate a Python virtual environment and activate it by running this command:\npython3 -m venv venv && source venv/bin/activate\nBash\nInstall the Python dependencies by running:\npip3 install -r requirements.txt\nBash\nSimulate an EV charge point boot and heartbeat notification\nThe Python script simulates some basic functionality of an EV charge point:\nSending a BootNotification, including attributes about the CP hardware\nSending Heartbeat messages based on a frequency instructed by the CPO (this is defined by the interval parameter returned in the response to the BootNotification)\nRun the Python script using the following command, making sure to replace the --url value with the AwsOcppGatewayStack.websocketURL returned from the cdk deployment:\npython3 simulate.py --url {websocket URL generated from the AWS OCPP Stack} --cp-id CP1 \nBash\nNote: we are using --cp-id CP1 which must match the value of the IoT Thing created above. If the --cp-id doesn\xe2\x80\x99t match the IoT Thing name, the connection will be rejected by the OCPP Gateway.\nA successful output should look like this:\n(venv) ev-charge-point-simulator % python3 simulate.py --url {websocket URL generated from the AWS OCPP Stack} --cp-id CP1 \nINFO:ocpp:CP1: send [2,""0678cb2a-a7a2-42bc-8037-d01164e77ac6"",""BootNotification"",{""chargingStation"":{""model"":""ABC 123 XYZ"",""vendorName"":""Acme Electrical Systems"",""firmwareVersion"":""10.9.8.ABC"",""serialNumber"":""CP1234567890A01"",""modem"":{""iccid"":""891004234814455936F"",""imsi"":""310410123456789""}},""reason"":""PowerUp""}]\nINFO:ocpp:CP1: receive message [3,""0678cb2a-a7a2-42bc-8037-d01164e77ac6"",{""currentTime"":""2023-02-16T19:00:18.630818"",""interval"":10,""status"":""Accepted""}]\nINFO:root:CP1: connected to central system\nINFO:root:CP1: heartbeat interval set to 10\nINFO:ocpp:CP1: send [2,""9b7933a7-5216-496d-9bb0-dae45014bb98"",""Heartbeat"",{}]\nINFO:ocpp:CP1: receive message [3,""9b7933a7-5216-496d-9bb0-dae45014bb98"",{""currentTime"":""2023-02-16T19:00:19.073675""}]\nBash\nThis exchange represents a successful simulation of a CP, first sending a BootNotification, followed by subsequent Heartbeat at the specified interval. The output includes both the simulated OCPP message sent from the CP to AWS IoT (prefixed send) and the response received from AWS (prefixed received message).\nTo simulate with a different CP, set a different value for the --cp-id argument.\nNote: if the --cp-id value doesn\xe2\x80\x99t have a correspondent IoT Thing the OCPP Gateway will reject the connection. Here is an unsuccessful example passing --cp-id CP2, which is not registered as a Thing in IoT:\n(venv) ev-charge-point-simulator % python3 simulate.py --url {websocket URL generated from the AWS OCPP Stack} --cp-id CP2 \nINFO:ocpp:CP2: send [2,""32dc5b6e-77b0-4105-b217-28e20b579ecc"",""BootNotification"",{""chargingStation"":{""model"":""ABC 123 XYZ"",""vendorName"":""Acme Electrical Systems"",""firmwareVersion"":""10.9.8.ABC"",""serialNumber"":""CP1234567890A01"",""modem"":{""iccid"":""891004234814455936F"",""imsi"":""310410123456789""}},""reason"":""PowerUp""}]\nERROR:root:CP2: received 1008 (policy violation) Charge Point CP2 not registered as an IoT Thing; then sent 1008 (policy violation) Charge Point CP2 not registered as an IoT Thing\nBash\nMonitor OCPP activity in the AWS Console\nMessages from and to the CP are brokered through AWS IoT Core. These messages utilize the MQTT publish-and-subscribe protocol. You can see these messages in the console.\nIn AWS Explorer, select your region and open AWS IoT Core, MQTT test client\nIn the test client, select the Subscribe to a topic tab, and subscribe to these two topics by entering these values in the Topic filter:\na. To view all messages from CP to AWS\n+/in\nBash\nb. To view all messages from AWS to CP\n+/out\nBash\nScreenshot: Subscribe to Topics\nRun the Python script to simulate a CP and watch the messages in the MQTT test client\nTrack EV Charge Point hardware attributes in device shadows\nWhen a CP sends a BootNotification, its hardware attributes are stored in a Device Shadow associated with the IoT Thing. You can see these attributes in the console.\nIn AWS Explorer, select your region and open AWS IoT Core, All devices, Things\nToggle the check box against the Thing created previously\nSelect the Device Shadows tab.\nSelect the Classic Shadow device shadow name hyperlink to see the Device Shadow document and the hardware attributes reported by the EV Charge Point:\n{\n  ""state"": {\n    ""reported"": {\n      ""chargingStation"": {\n        ""model"": ""ABC 123 XYZ"",\n        ""vendorName"": ""Acme Electrical Systems"",\n        ""firmwareVersion"": ""10.9.8.ABC"",\n        ""serialNumber"": ""CP1234567890A01"",\n        ""modem"": {\n          ""iccid"": ""891004234814455936F"",\n          ""imsi"": ""310410123456789""\n        }\n      },\n      ""reason"": ""PowerUp""\n    }\n  }\n}\nJSON\nScreenshot: IoT Thing shadow document\nSimulate different CP hardware attributes by passing these arguments into the simulate.py script and verify their affect on the Device Shadow:\n--cp-serial \xe2\x80\x93 to set the serial number\n--cp-model \xe2\x80\x93 to set the model identification\n--cp-version \xe2\x80\x93 to set the firmware version\n--cp-vendor \xe2\x80\x93 to set the vendor name\n'"
9,Ingesting industrial media to Amazon Kinesis Video Streams using AWS IoT Greengrass V2 components,b'Costin B\xc4\x83dici',2023-04-17T17:33:20+00:00,https://d2908q01vomqb2.cloudfront.net/f6e1126cedebf23e1463aee73f9df08783640400/2023/03/21/Ingesting-industrial-media-to-Amazon-Kinesis-Video-Streams-using-AWS-IoT-Greengrass-V2-components.jpg-1024x576.png,https://aws.amazon.com/blogs/iot/ingesting-media-to-kinesis-video-streams/,"b'Introduction\nOrganizations install hundreds of Internet Protocol (IP) cameras to increase security by surveilling indoor and outdoor spaces. This is a common need for manufacturing plant floors across industries such as automotive, commercial, Oil & Gas, public safety, and agri-tech. Companies connect cameras to the cloud to create a centralized view of their siloed data and also to add digital twin capabilities to their facilities. In this post, we will discuss how to use AWS IoT Greengrass V2 components to package and deploy an Amazon Kinesis Video Streams stream uploader that can do live-streaming, on-demand video upload, and local caching, thus facilitating use cases that require ingestion of live audio and video streams from pre-installed IP cameras.\nKinesis Video Streams is a fully managed AWS service that you can use to stream live video from devices to AWS, build applications for real-time video processing, perform batch video analytics, and more. You can use the Kinesis Video Streams service to ingest video and non-video data from many different sources like smartphones, security cameras, web cameras, drones, thermal imagery, and audio. Recently, we released an AWS IoT Greengrass component for Kinesis Video Streams that lets you stream media from your existing devices. The edge connector for the Kinesis Video Streams component (aws.iot.EdgeConnectorForKVS) reads video feeds from local IP cameras using Real Time Streaming Protocol (RTSP) and publishes the streams to the Kinesis Video Streams endpoint.\nSolution overview\nA company that develops smart building solutions is interested in building an application that ingests hundreds of video streams from the building entry, access control areas, and security gates. The company might be looking to incorporate video feeds in to a digital twin application using AWS IoT TwinMaker, along with using the AWS IoT TwinMaker application plugin for Grafana dashboard to request uploading of videos and checking historical video timelines.\nIt can be challenging to refactor the existing cameras to stream to an endpoint. Instead, you can deploy an AWS IoT Greengrass edge gateway and an AWS IoT Greengrass edge connector for Kinesis Video Streams component to ingest data from these cameras. The component connects to IP cameras within the same network and streams the video feed to Kinesis Video Streams. On the consumption side, you can use an application to read from the Kinesis Video Streams endpoint to act as a client. This component supports features like edge video caching, scheduled video recording, scheduled video uploading, live streaming, and historical video uploading to Kinesis Video Streams. The edge connector component provides a fully working AWS IoT Greengrass V2 component for video ingestion that can be customized based on needs.\nThe number of cameras that can be connected to this component per AWS IoT Greengrass hub is dependent on the compute power of the underlying hardware, network bandwidth, and AWS IoT SiteWise child assets used to store the configuration of the connector (currently this limit is 2000, refer to the documentation for details on child asset quotas). This architecture assumes that there is a stable network connection between the AWS IoT Greengrass device and AWS, with sufficient bandwidth for streaming the media.\nFigure: Ingesting video feed from IP cameras using AWS IoT Greengrass v2 components\nDeployment of AWS IoT Greengrass Core at the edge device. This device will be responsible for running the edge connector and interfacing with the cameras. The Greengrass Core software can be deployed on a Linux device, such as a Raspberry Pi or a Windows device. This device will eventually run the edge connector  Kinesis Video Streams component. Refer documentation for more details on how to setup AWS IoT Greengrass.\nInstall GStreamer version 1.18.4 or later on the edge device.\nOnce the edge device is setup, use the AWS IoT Greengrass service to deploy the edge connector for Kinesis Video Streams component. Edit the configuration page with the details specific to your deployment. Refer documentation to learn more about how to deploy components to AWS IoT Greengrass.\nOnce the edge connector for Kinesis Video Streams component is deployed, the configuration for the component is stored in AWS IoT SiteWise and AWS Secrets Manager. AWS IoT SiteWise stores two types of assets: theEdgeConnectorForKVSHub asset contains the asset name that identifies the unique hub where the connector is running and the EdgeConnectorForKVSCamera contains the properties specific to the cameras, like cron expression to start streaming and recording. For details, refer the GitHub page about the configuration parameters needed for this service .\nThe edge connector for Kinesis Video Streams ingests data from the camera feeds. There is an option here to add local storage as well as stream them to the Kinesis Video Streams endpoint.\nOn the client side, you can build your own custom applications to consume data from the Kinesis Video Streams endpoint. As an example, you can trigger live streaming video when you detect a motion.\nFor detailed steps on implementing this architecture and above steps, refer to github documentation.\nDetailed edge architecture\nFigure: Architecture for ingesting IP video cameras feed in Amazon Kinesis Video Stream\nThe edge architecture has three modules: controller, video recorder, and video uploader (see the previous diagram). While the default setting of the Kinesis Video Streams connector component is to stream the video, it has an optional functionality of recording video on the file system for local storage. The controller acts as a broker between the recorder and the uploader. It also facilitates communication between the two. The controller first initiates a pair of piped input and output stream objects. The video recorder retrieves stream data from the camera and puts the data into the piped output stream. Finally, the video uploader takes stream data from the piped input stream, then uploads the data to Kinesis Video Streams.\nScaling the solution\nNext, we\xe2\x80\x99ll look at sizing and limits to see how the solution scales. In the architecture, the edge connector for Kinesis Video Streams component and Greengrass Core do not have any scaling limitations. As the solution uses AWS IoT SiteWise to manage the RTSP camera configuration, the only hard limit is from the AWS IoT SiteWise child assets quota, which is fewer than 2000 child assets per parent asset. The number of cameras that can be supported by the edge device/Hub only depends on its hardware configurations. If there is enough network bandwidth and hardware capacity, the AWS IoT Greengrass device can support more cameras. From our internal testing, we tested 10+ cameras connected to the same edge device to ingest the feeds without any issues. Refer to the documentation Kinesis Video Streams API limits and quotas.\nBelow are some sample edge device configurations and the number of video streams they can support for optimal performance:\nA small instance (like Raspberry Pi 4 Model B) with 2GB RAM and 16GB SSD can support up to 2 1080p HD RTSP cameras uploading to the cloud at the same time with a network speed of 100 MBPS.\nA medium instance (like NVIDIA Jetson Nano Developer Kit) with 4GB RAM and 16GB SSD can support up to 4 1080p HD RTSP cameras uploading to the cloud at the same time over a network speed of 100 MBPS.\nA large instance (like Intel NUC) with 25GB RAM and 1T SSD can support up to 24 1080p HD RTSP cameras uploading to the cloud at the same time with a network speed of 600 MBPS.\nThis solution is primarily memory dependent, hence compute resources like CPU and GPU type and capacity are less relevant.\nClean up\nIf you used the GitHub link to implement this architecture, make sure to use the below steps to clean-up the resources to avoid incurring cost.\nUninstall Greengrass core software from the edge device\nDelete Kinesis Video Stream:\nOpen the Kinesis Video Streams console\nChoose Video streams in the left-hand menu and select the video stream\nChoose Delete video stream in the upper right corner of the screen\nA confirmation screen will appear. Enter Delete in the field and select Delete.\n'"
